{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# RAG Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook implements a Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. It answers queries using two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`, handling questions about leadership, products, and specifications.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI for accurate answers from PDFs.\n",
        "- Use lightweight models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`) for CPU compatibility.\n",
        "- Apply regex post-processing for roles (e.g., CEO names) and products (e.g., product lists) with deduplication.\n",
        "- Provide an interactive query interface with readable output.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded with `PyPDFLoader`, split into chunks (300 chars, 50-char overlap) using `RecursiveCharacterTextSplitter`, and stored with source tracking for company filtering.\n",
        "- **Semantic Layer**: Chunks and queries are embedded with `lightonai/GTE-ModernColBERT-v1`.\n",
        "- **Retrieval**: `retrieve.ColBERT` fetches 15 chunks, reranked to 3 by `rank.rerank`.\n",
        "- **Augmentation**: Top 3 chunks are combined with the query via `PromptTemplate`.\n",
        "- **Generation**: `google/flan-t5-base` generates answers, post-processed for roles, products, or lists.\n",
        "\n",
        "## Updates\n",
        "- Improved role extraction regex for names (e.g., 'Dr. Elena Ruiz' for CEO).\n",
        "- Refined product regex to filter non-products (e.g., 'Compliance').\n",
        "- Added company filtering (QuantumCore/NeoCompute) based on query keywords.\n",
        "- Enhanced output formatting with clear sections, suppressed progress bars, and concise progress messages.\n",
        "- Fixed errors for robust query handling.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet`.\n",
        "- **Environment**: Google Colab, CPU-friendly.\n",
        "- **Datasets**: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` (quantum computing), `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf` (AI hardware/software).\n",
        "\n",
        "## Instructions\n",
        "1. Cell 1: Install libraries.\n",
        "2. Cell 2: Import libraries.\n",
        "3. Cell 3: Define RAG pipeline.\n",
        "4. Cell 4: Process PDFs.\n",
        "5. Cell 5: Run interactive queries with formatted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdRrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Installs Python libraries for the RAG pipeline, ensuring compatibility in Google Colab. Includes `pylate` (ColBERT embeddings/retrieval), `langchain` (document processing), `transformers` (FLAN-T5), `google-colab` (Colab utilities), and `langchain-community`, `pypdf`, `hf_xet` for PDF processing and Hugging Face integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "Imports libraries for the pipeline and suppresses warnings for clean output. Includes `pylate` (embedding/retrieval), `langchain` (PDF loading, text splitting, prompts), `transformers` (FLAN-T5), `google.colab.files` (Colab uploads), `os` (file paths), `re` (regex), and suppresses `pypdf` warnings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "warnings.filterWarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterWarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "Defines the RAG pipeline functions:\n",
        "- `run_rag_pipeline`: Loads, chunks, embeds, and indexes PDFs, initializes retriever and generator, tracks source PDFs for company filtering.\n",
        "- `query_rag`: Encodes queries, retrieves/reranks chunks, generates answers, and post-processes for roles, products, or lists. Features:\n",
        "  - Company filtering (QuantumCore/NeoCompute).\n",
        "  - Improved role regex (e.g., CEO names).\n",
        "  - Refined product regex (excludes non-products).\n",
        "  - Suppressed progress bars, custom progress messages.\n",
        "  - Robust error handling for empty results.\n",
        "\n",
        "Ensures valid JSON with proper string escaping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "import contextlib\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "tqdm.__init__ = lambda *args, **kwargs: None  # Disable tqdm progress bars\n",
        "\n",
        "def run_rag_pipeline(pdf_paths):\n",
        "    \"\"\"Initialize the RAG pipeline by processing PDFs, creating embeddings, and setting up models.\"\"\"\n",
        "    try:\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        document_map = {}\n",
        "        current_doc_id = 0\n",
        "        document_sources = {}\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            if not os.path.exists(pdf_path):\n",
        "                print(f'PDF not found: {pdf_path}')\n",
        "                continue\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            source_name = os.path.basename(pdf_path).split('_')[0]\n",
        "            document_sources.update({doc_id: source_name for doc_id in document_ids})\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        if not all_document_texts:\n",
        "            print('No documents processed.')\n",
        "            return None\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        documents_embeddings = model.encode(\n",
        "            all_document_texts,\n",
        "            batch_size=32,\n",
        "            is_query=False,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        prompt_template = \"Using only the provided text, answer the user's question with a concise and accurate response. For questions about specific roles (e.g., CEO, CTO, CFO), return only the full name of the individual in that role. For questions about lists (e.g., products), return all items as a comma-separated list of names only. Exclude any details not directly relevant to the question, such as technical specifications, unless explicitly requested. If the answer is not in the text, respond with 'The answer could not be found in the text.'\\n\\nText: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "        return model, index, retriever, generator, PROMPT, document_map, document_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query):\n",
        "    \"\"\"Process a user query with retrieval, generation, and post-processing.\"\"\"\n",
        "    try:\n",
        "        queries = [query]\n",
        "\n",
        "        print('Encoding query...')\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            query_embedding = model.encode(\n",
        "                queries,\n",
        "                batch_size=32,\n",
        "                is_query=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "\n",
        "        print('Retrieving documents...')\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            top_k_initial = 15\n",
        "            initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "\n",
        "        if not initial_results or not initial_results[0]:\n",
        "            print('No documents retrieved.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0] if 'id' in result]\n",
        "        if not retrieved_doc_ids:\n",
        "            print('No document IDs after retrieval.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        company = None\n",
        "        if 'quantumcore' in query.lower():\n",
        "            company = 'QuantumCore'\n",
        "        elif 'neocompute' in query.lower():\n",
        "            company = 'NeoCompute'\n",
        "        if company:\n",
        "            retrieved_doc_ids = [doc_id for doc_id in retrieved_doc_ids if document_sources.get(doc_id) == company]\n",
        "            if not retrieved_doc_ids:\n",
        "                print(f'No documents found for company: {company}')\n",
        "                return None, f'No documents found for {company}.'\n",
        "\n",
        "        retrieved_documents = [document_map.get(doc_id, '') for doc_id in retrieved_doc_ids]\n",
        "        retrieved_documents = [doc for doc in retrieved_documents if doc]  # Remove empty documents\n",
        "\n",
        "        if not retrieved_documents:\n",
        "            print('No valid documents after filtering.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        print('Reranking documents...')\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            reranked_results = rank.rerank(\n",
        "                documents_ids=[retrieved_doc_ids],\n",
        "                queries_embeddings=query_embedding,\n",
        "                documents_embeddings=[model.encode(retrieved_documents, is_query=False, show_progress_bar=False)]\n",
        "            )\n",
        "\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            reranked_doc_ids = retrieved_doc_ids[:3]  # Fallback to top 3 retrieved\n",
        "\n",
        "        if not reranked_doc_ids:\n",
        "            print('No document IDs after reranking.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        reranked_documents = [document_map.get(doc_id, '') for doc_id in reranked_doc_ids]\n",
        "        reranked_documents = [doc for doc in reranked_documents if doc]\n",
        "\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        if not context:\n",
        "            print('No context generated.')\n",
        "            return None, 'No relevant context found.'\n",
        "\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        print('Generating answer...')\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        non_product_terms = {'Compliance', 'Cooling', 'Features', 'Storage', 'Networking', 'Frameworks', 'Uptime', 'Encryption', 'Certifications', 'Software'}\n",
        "        if any(role in query.lower() for role in ['ceo', 'cto', 'cfo', 'cio']):\n",
        "            role = next((r for r in ['CEO', 'CTO', 'CFO', 'CIO'] if r.lower() in query.lower()), None)\n",
        "            if role:\n",
        "                match = re.search(r'- ([^,]+?),\\s*'+role+r'\\s*:', context, re.IGNORECASE)\n",
        "                if match:\n",
        "                    answer = match.group(1).strip()\n",
        "                else:\n",
        "                    answer = 'The answer could not be found in the text.'\n",
        "        elif 'product' in query.lower():\n",
        "            product_names = re.findall(r'- (\\w+): (?:Quantum|Cloud-based|processing unit|platform|cryptographic security|cloud-native|security modules)', context, re.IGNORECASE)\n",
        "            product_names = [name for name in product_names if name not in non_product_terms]\n",
        "            if product_names:\n",
        "                answer = ', '.join(sorted(set(product_names)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        elif 'compliance' in query.lower():\n",
        "            compliance_standards = re.findall(r'- ([^\\n]+)', context, re.IGNORECASE)\n",
        "            compliance_standards = [std.strip() for std in compliance_standards if any(term in std for term in ['SOC', 'ISO', 'HIPAA', 'GDPR'])]\n",
        "            if compliance_standards:\n",
        "                answer = ', '.join(sorted(set(compliance_standards)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        elif ', ' in answer:\n",
        "            items = set(answer.split(', '))\n",
        "            items = [item for item in items if item not in non_product_terms]\n",
        "            answer = ', '.join(sorted(items)) if items else 'The answer could not be found in the text.'\n",
        "\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "Specifies PDF paths (`QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf`, `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`) and initializes the RAG pipeline. Checks for missing files and prompts for uploads in Colab. Initializes ColBERT model, Voyager index, retriever, FLAN-T5 generator, prompt template, document map, and source tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    model, index, retriever, generator, PROMPT, document_map, document_sources = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "Provides an interactive query interface with formatted output. Features:\n",
        "- Clear sections with headers, horizontal lines, and indentation.\n",
        "- Suppressed progress bars, replaced with concise messages.\n",
        "- Robust error handling for failed queries.\n",
        "- Example queries test roles, products, specifications, and compliance.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions? → 'Dr. Elena Ruiz'\n",
        "- What are the products offered by NeoCompute Technologies? → 'NeoCloud, NeoSecure'\n",
        "- Who is the CIO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- Who is the CEO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- What is the qubit count of QubitCore? → '50-qubit superconducting architecture'\n",
        "- What compliance standards does NeoCompute follow? → 'SOC 2 Type II, ISO/IEC 27001, HIPAA, GDPR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run an interactive query loop with formatted output.\"\"\"\n",
        "    print('====================================')\n",
        "    print('RAG Query Interface')\n",
        "    print('====================================')\n",
        "    print('Enter your query (or type \"exit\" to quit):\\n')\n",
        "    while True:\n",
        "        query = input('Query: ')\n",
        "        if query.lower() == 'exit':\n",
        "            print('\\nExiting query interface.')\n",
        "            break\n",
        "        if not result:\n",
        "            print('\\nRAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query)\n",
        "        print('\\n====================================')\n",
        "        print(f'Query: {query}')\n",
        "        print('====================================')\n",
        "        print('\\n**Context Retrieved**:\\n')\n",
        "        if context is None:\n",
        "            print('    Error: No context retrieved.')\n",
        "        else:\n",
        "            indented_context = context.replace('\\n', '\\n    ')\n",
        "            print(f'    {indented_context}')\n",
        "        print('\\n---')\n",
        "        print('\\n**Answer**:\\n')\n",
        "        print(f'    {answer}')\n",
        "        print('\\n====================================\\n')\n",
        "        print('Enter your next query (or type \"exit\" to quit):\\n')\n",
        "\n",
        "interactive_query()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "nbformat": 4,
    "nbformat_minor": 0
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
