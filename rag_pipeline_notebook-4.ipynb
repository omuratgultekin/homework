{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook implements a minimal Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. The system answers user queries by leveraging content from two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`. It demonstrates versatility in handling varied queries (e.g., leadership roles, product lists, technical specifications) using a lightweight, CPU-friendly setup suitable for Google Colab.\n",
        "\n",
        "## Objective\n",
        "- **Purpose**: Combine retrieval and generative AI to provide accurate, context-grounded answers from PDF content.\n",
        "- **Resource Efficiency**: Use small models (`lightonai/GTE-ModernColBERT-v1` for embeddings, `google/flan-t5-base` for generation) to ensure compatibility with CPU environments.\n",
        "- **Post-Processing**: Apply minimal regex-based post-processing for role-based queries (e.g., extracting CEO names) and product queries (e.g., listing product names), with deduplication to ensure clean outputs.\n",
        "- **Interactivity**: Support an interactive query interface for demo purposes, with example queries to showcase functionality.\n",
        "\n",
        "## Architecture\n",
        "The pipeline follows a modular RAG design:\n",
        "- **Knowledge Base**: PDFs are loaded using `PyPDFLoader` and split into chunks (300 characters, 50-character overlap) with `RecursiveCharacterTextSplitter`. Chunks are stored in a dictionary mapping document IDs to text, with source tracking for company-specific filtering.\n",
        "- **Semantic Layer**: Text chunks and queries are embedded into dense vectors using `lightonai/GTE-ModernColBERT-v1` for semantic similarity comparison.\n",
        "- **Retrieval System**: `retrieve.ColBERT` fetches the top 15 relevant chunks based on query embeddings, which are reranked to the top 3 using `rank.rerank` for improved relevance.\n",
        "- **Augmentation**: The top 3 chunks (up to 600 characters) are combined with the query via a `PromptTemplate` to create a contextualized input for the generative model.\n",
        "- **Generation**: `google/flan-t5-base` produces concise answers, with post-processing to extract names for role queries (e.g., CEO), list products for product queries, or deduplicate comma-separated lists.\n",
        "- **Fixes Implemented**:\n",
        "  - **Role Extraction**: Improved regex to handle formatting variations and case sensitivity for reliable name extraction (e.g., 'Dr. Elena Ruiz' for CEO).\n",
        "  - **Product Extraction**: Refined regex to target quantum-related products and filter out non-product terms (e.g., 'Compliance').\n",
        "  - **Company Filtering**: Added source tracking to filter chunks by company (QuantumCore or NeoCompute) based on query keywords.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: Requires `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet` for PDF processing, embedding, retrieval, and generation.\n",
        "- **Environment**: Designed for Google Colab with CPU, ensuring accessibility without GPU requirements.\n",
        "- **Datasets**: Processes `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` (quantum computing company details) and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf` (assumed similar content).\n",
        "\n",
        "## Instructions\n",
        "1. **Cell 1**: Install required Python libraries to set up the environment.\n",
        "2. **Cell 2**: Import libraries and suppress warnings for cleaner output.\n",
        "3. **Cell 3**: Define the RAG pipeline functions (`run_rag_pipeline` and `query_rag`) with improved logic.\n",
        "4. **Cell 4**: Load and process the PDFs, initializing the pipeline with models and indexes.\n",
        "5. **Cell 5**: Run an interactive query interface to test the pipeline with example or custom queries.\n",
        "\n",
        "The pipeline combines chunks from both PDFs into a single knowledge base but filters by company when specified in queries, ensuring relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdKrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "This cell installs the necessary Python libraries for the RAG pipeline. It ensures compatibility in a clean Google Colab environment by installing `pylate` (for ColBERT embeddings and retrieval), `langchain` (for document loading and splitting), `transformers` (for the FLAN-T5 model), `google-colab` (for Colab utilities), and additional dependencies (`langchain-community`, `pypdf`, `hf_xet`) for PDF processing and Hugging Face integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "# Install core libraries for RAG pipeline (pylate for ColBERT, langchain for document processing, transformers for generation)\n",
        "!pip install pylate langchain transformers google-colab\n",
        "# Install additional dependencies for PDF loading and Hugging Face integration\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "This cell imports the required Python libraries for the pipeline and suppresses warnings to ensure cleaner output in Colab. Key libraries include:\n",
        "- `pylate` for ColBERT-based embedding and retrieval (`models`, `indexes`, `retrieve`, `rank`).\n",
        "- `langchain` for PDF loading (`PyPDFLoader`), text splitting (`RecursiveCharacterTextSplitter`), and prompt creation (`PromptTemplate`).\n",
        "- `transformers` for the FLAN-T5 model (`pipeline`).\n",
        "- `google.colab.files` for handling file uploads in Colab.\n",
        "- `os`, `re` for file path handling and regex post-processing.\n",
        "- Warnings from `pypdf` are suppressed to avoid cluttering the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank  # For ColBERT embedding, indexing, and retrieval\n",
        "from langchain.document_loaders import PyPDFLoader  # For loading PDF documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into chunks\n",
        "from langchain.prompts import PromptTemplate  # For creating prompt templates\n",
        "from google.colab import files  # For file uploads in Colab\n",
        "import os  # For file path handling\n",
        "from transformers import pipeline  # For FLAN-T5 text generation\n",
        "import re  # For regex-based post-processing\n",
        "\n",
        "# Suppress warnings from pypdf for cleaner output\n",
        "warnings.filterWarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterWarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "This cell defines the core functions of the RAG pipeline:\n",
        "- **`run_rag_pipeline`**: Processes PDFs by loading, chunking, embedding, and indexing them, then initializes the retriever and generator. It now tracks the source PDF for each chunk to enable company-specific filtering.\n",
        "- **`query_rag`**: Handles user queries by encoding them, retrieving and reranking relevant chunks, augmenting the query with context, generating an answer, and applying post-processing. Fixes include:\n",
        "  - **Company Filtering**: Filters chunks by company (QuantumCore or NeoCompute) based on query keywords.\n",
        "  - **Role Extraction**: Uses an improved regex to extract names for roles (e.g., CEO) reliably.\n",
        "  - **Product Extraction**: Uses a refined regex to target quantum-related products and filters out non-product terms.\n",
        "  - **Robust Post-Processing**: Ensures accurate deduplication and fallback to raw generated answers when needed.\n",
        "\n",
        "The pipeline is designed to be robust, handling errors gracefully and providing clear feedback if processing fails. All strings (e.g., prompt template, regex patterns) are properly escaped to ensure valid JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(pdf_paths):\n",
        "    \"\"\"Initialize the RAG pipeline by processing PDFs, creating embeddings, and setting up models.\"\"\"\n",
        "    try:\n",
        "        # Initialize storage for document texts, IDs, and sources\n",
        "        all_document_texts = []  # Store text chunks from all PDFs\n",
        "        all_document_ids = []  # Store unique IDs for each chunk\n",
        "        document_map = {}  # Map IDs to text chunks\n",
        "        current_doc_id = 0  # Track ID increments across PDFs\n",
        "        document_sources = {}  # Track source PDF (QuantumCore or NeoCompute) for each chunk\n",
        "\n",
        "        # Initialize text splitter (300 chars, 50-char overlap) for chunking PDFs\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            # Load PDF using PyPDFLoader\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            # Split documents into chunks\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            # Assign unique IDs to chunks\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            # Update document map with ID-to-text mapping\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            # Track source PDF (e.g., 'QuantumCore' or 'NeoCompute') for filtering\n",
        "            source_name = os.path.basename(pdf_path).split('_')[0]\n",
        "            document_sources.update({doc_id: source_name for doc_id in document_ids})\n",
        "            # Extend lists with current PDF's chunks and IDs\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        # Load ColBERT model for embedding\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        # Initialize Voyager index for storing embeddings\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        # Embed document chunks using ColBERT\n",
        "        documents_embeddings = model.encode(\n",
        "            all_document_texts,\n",
        "            batch_size=32,\n",
        "            is_query=False,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        # Add embeddings to index with corresponding IDs\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        # Initialize ColBERT retriever\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        # Initialize FLAN-T5 model for text generation\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        # Define prompt template for answer generation (properly escaped for JSON)\n",
        "        prompt_template = \"Using only the provided text, answer the user's question with a concise and accurate response. For questions about specific roles (e.g., CEO, CTO, CFO), return only the full name of the individual in that role. For questions about lists (e.g., products), return all items as a comma-separated list of names only. Exclude any details not directly relevant to the question, such as technical specifications, unless explicitly requested. If the answer is not in the text, respond with 'The answer could not be found in the text.'\\n\\nText: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "        # Return initialized components, including document_sources for company filtering\n",
        "        return model, index, retriever, generator, PROMPT, document_map, document_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query):\n",
        "    \"\"\"Process a user query by retrieving relevant chunks, generating an answer, and applying post-processing.\"\"\"\n",
        "    try:\n",
        "        queries = [query]\n",
        "\n",
        "        # Encode query using ColBERT\n",
        "        query_embedding = model.encode(\n",
        "            queries,\n",
        "            batch_size=32,\n",
        "            is_query=True,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "        # Retrieve top 15 documents based on query embedding\n",
        "        top_k_initial = 15\n",
        "        initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0]]\n",
        "\n",
        "        # Filter documents by company if specified in query (e.g., 'QuantumCore' or 'NeoCompute')\n",
        "        company = None\n",
        "        if 'quantumcore' in query.lower():\n",
        "            company = 'QuantumCore'\n",
        "        elif 'neocompute' in query.lower():\n",
        "            company = 'NeoCompute'\n",
        "        if company:\n",
        "            retrieved_doc_ids = [doc_id for doc_id in retrieved_doc_ids if document_sources.get(doc_id) == company]\n",
        "        retrieved_documents = [document_map[doc_id] for doc_id in retrieved_doc_ids]\n",
        "\n",
        "        # Rerank documents to select top 3 most relevant\n",
        "        reranked_results = rank.rerank(\n",
        "            documents_ids=[retrieved_doc_ids],\n",
        "            queries_embeddings=query_embedding,\n",
        "            documents_embeddings=[model.encode(retrieved_documents, is_query=False)]\n",
        "        )\n",
        "\n",
        "        # Extract reranked document IDs\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            reranked_doc_ids = retrieved_doc_ids\n",
        "\n",
        "        reranked_documents = [document_map[doc_id] for doc_id in reranked_doc_ids]\n",
        "\n",
        "        # Create context from top 3 reranked documents (max 600 characters)\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        # Generate answer using FLAN-T5\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        # Post-processing for role, product, or list-based queries\n",
        "        non_product_terms = {'Compliance', 'Cooling', 'Features', 'Storage', 'Networking', 'Frameworks', 'Uptime', 'Encryption', 'Certifications', 'Software'}\n",
        "        if any(role in query.lower() for role in ['ceo', 'cto', 'cfo']):\n",
        "            # Extract role (e.g., CEO) and use regex to find name\n",
        "            role = next((r for r in ['CEO', 'CTO', 'CFO'] if r.lower() in query.lower()), None)\n",
        "            if role:\n",
        "                # Improved regex: non-greedy, handles spaces, case-insensitive\n",
        "                match = re.search(r'- ([^,]+?),\\s*'+role+r'\\s*:', context, re.IGNORECASE)\n",
        "                if match:\n",
        "                    answer = match.group(1).strip()\n",
        "                else:\n",
        "                    answer = 'The answer could not be found in the text.'\n",
        "        elif 'product' in query.lower():\n",
        "            # Extract product names with regex targeting quantum-related terms\n",
        "            product_names = re.findall(r'- (\\w+): (?:Quantum|Cloud-based|processing unit|platform|cryptographic security)', context, re.IGNORECASE)\n",
        "            # Filter out non-product terms\n",
        "            product_names = [name for name in product_names if name not in non_product_terms]\n",
        "            if product_names:\n",
        "                # Sort and deduplicate product names\n",
        "                answer = ', '.join(sorted(set(product_names)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        elif ', ' in answer:\n",
        "            # Handle comma-separated lists (e.g., compliance standards)\n",
        "            items = set(answer.split(', '))\n",
        "            # Filter out non-product terms\n",
        "            items = [item for item in items if item not in non_product_terms]\n",
        "            answer = ', '.join(sorted(items)) if items else 'The answer could not be found in the text.'\n",
        "\n",
        "        # Debug logging (commented out for production)\n",
        "        # print(f'Debug: Query={query}, Context={context[:100]}..., Raw Answer={response}, Final Answer={answer}')\n",
        "\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "This cell specifies the paths to the PDF datasets (`QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`) and initializes the RAG pipeline by calling `run_rag_pipeline`. If running locally, ensure the PDFs are in the `/data` directory. In Google Colab, the cell checks for missing files and prompts the user to upload them. The pipeline is initialized with the ColBERT model, Voyager index, retriever, FLAN-T5 generator, prompt template, document map, and source tracking for company-specific filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "# Define PDF paths (modify as needed for local environment)\n",
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "# Check if PDFs exist; if not, prompt for upload in Colab\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "\n",
        "# Initialize the RAG pipeline\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    # Unpack pipeline components, including document_sources\n",
        "    model, index, retriever, generator, PROMPT, document_map, document_sources = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "This cell provides an interactive interface to query the RAG system. Users can enter custom queries or use provided examples. The system retrieves relevant chunks, generates an answer, and displays both the context and response. Example queries test various aspects of the pipeline:\n",
        "- Role queries (e.g., CEO name) use regex to extract full names.\n",
        "- Product queries list product names, with fixes to exclude non-products.\n",
        "- Specification queries (e.g., qubit count) extract specific details.\n",
        "- Compliance queries return lists of standards, deduplicated and filtered.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions? → Expected: 'Dr. Elena Ruiz'\n",
        "- What are the products offered by NeoCompute Technologies? → Expected: 'QubitCore, QuantumNet' (based on prior context)\n",
        "- What is the qubit count of QubitCore? → Expected: '50-qubit superconducting architecture'\n",
        "- What compliance standards does NeoCompute follow? → Expected: 'FIPS 140-3, GDPR' (based on prior context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run an interactive query loop to test the RAG pipeline.\"\"\"\n",
        "    print('Enter your query (or type \"exit\" to quit):')\n",
        "    while True:\n",
        "        query = input('Query: ')\n",
        "        if query.lower() == 'exit':\n",
        "            print('Exiting query interface.')\n",
        "            break\n",
        "        if not result:\n",
        "            print('RAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "        # Process query and display context and answer\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query)\n",
        "        print('\\n**Context Retrieved**:\\n', context)\n",
        "        print('\\n**Answer**:\\n', answer, '\\n')\n",
        "\n",
        "# Start the interactive query interface\n",
        "interactive_query()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
