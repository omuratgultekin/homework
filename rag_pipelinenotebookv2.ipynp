{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook demonstrates a minimal Retrieval-Augmented Generation (RAG) pipeline designed for a take-home project interview.\n",
        "\n",
        "The system answers user queries based on two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v3.pdf`, showcasing versatility for varied queries.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI to answer queries grounded in PDF content.\n",
        "- Use small, CPU-friendly models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`).\n",
        "- Minimize post-processing with regex for roles/products and deduplication.\n",
        "- Support interactive querying for demo purposes.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded (`PyPDFLoader`), split into chunks (`RecursiveCharacterTextSplitter`), and stored in memory.\n",
        "- **Semantic Layer**: Chunks and queries are embedded using `lightonai/GTE-ModernColBERT-v1` for semantic comparison.\n",
        "- **Retrieval System**: `retrieve.ColBERT` fetches top 15 chunks, reranked to top 3 (`rank.rerank`).\n",
        "- **Augmentation**: Retrieved chunks (600-char limit) are combined with the query via `PromptTemplate`.\n",
        "- **Generation**: `google/flan-t5-base` generates answers, post-processed with regex and deduplication.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google.colab`, `pypdf`, `hf_xet`.\n",
        "- **Environment**: Google Colab with CPU.\n",
        "- **Datasets**: PDFs in `/data` folder or uploaded manually.\n",
        "\n",
        "## How to Run\n",
        "1. Run Cell 1 to install dependencies.\n",
        "2. Run Cell 2 to import libraries and define the pipeline.\n",
        "3. Run Cell 3 to upload PDFs or specify paths.\n",
        "4. Run Cell 4 to process PDFs and initialize the pipeline.\n",
        "5. Run Cell 5 to query the pipeline interactively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdKrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Install required libraries for the RAG pipeline. This ensures the notebook runs in a clean Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries and Define RAG Pipeline\n",
        "\n",
        "This cell imports libraries, suppresses warnings, and defines the RAG pipeline as a modular class-based structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "import re\n",
        "from pylate import models, indexes, retrieve, rank\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "from transformers import pipeline\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterWarnings('ignore', category=DeprecationWarning, module='pypdf._reader')\n",
        "\n",
        "# Configuration dictionary for easy parameter tweaking\n",
        "CONFIG = {\n",
        "    'chunk_size': 300,\n",
        "    'chunk_overlap': 50,\n",
        "    'model_name': 'lightonai/GTE-ModernColBERT-v1',\n",
        "    'index_folder': 'pylate-index',\n",
        "    'index_name': 'pdf_index',\n",
        "    'max_context_length': 600,\n",
        "    'top_k_initial': 15,\n",
        "    'batch_size': 32,\n",
        "    'max_length': 300\n",
        "}\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Initialize the RAG pipeline with configuration.\"\"\"\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.index = None\n",
        "        self.retriever = None\n",
        "        self.generator = None\n",
        "        self.prompt = None\n",
        "        self.document_map = {}\n",
        "\n",
        "    def load_and_chunk_pdf(self, pdf_path):\n",
        "        \"\"\"Load and chunk a PDF into text segments.\"\"\"\n",
        "        if not os.path.exists(pdf_path):\n",
        "            print(f'PDF not found: {pdf_path}')\n",
        "            return None, None, None\n",
        "        try:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            if not documents:\n",
        "                print(f'No content found in PDF: {pdf_path}')\n",
        "                return None, None, None\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.config['chunk_size'],\n",
        "                chunk_overlap=self.config['chunk_overlap']\n",
        "            )\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [(chunk.page_content, {'pdf': pdf_path, 'page': chunk.metadata['page']}) for chunk in chunks]\n",
        "            document_ids = [str(i) + '_' + os.path.basename(pdf_path) for i in range(len(document_texts))]\n",
        "            document_map = {doc_id: text for doc_id, (text, _) in zip(document_ids, document_texts)}\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "            return document_texts, document_ids, document_map\n",
        "        except Exception as e:\n",
        "            print(f'Error processing PDF {pdf_path}: {e}')\n",
        "            return None, None, None\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize the ColBERT model for embedding.\"\"\"\n",
        "        try:\n",
        "            self.model = models.ColBERT(model_name_or_path=self.config['model_name'])\n",
        "            print(f'Loaded model: {self.config[\"model_name\"]}')\n",
        "        except Exception as e:\n",
        "            print(f'Failed to load ColBERT model {self.config[\"model_name\"]}: {e}')\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def create_index(self, document_texts, document_ids):\n",
        "        \"\"\"Create and populate the Voyager index with document embeddings.\"\"\"\n",
        "        try:\n",
        "            self.index = indexes.Voyager(\n",
        "                index_folder=self.config['index_folder'],\n",
        "                index_name=self.config['index_name'],\n",
        "                override=True\n",
        "            )\n",
        "            document_texts_only = [text for text, _ in document_texts]\n",
        "            documents_embeddings = self.model.encode(\n",
        "                document_texts_only,\n",
        "                batch_size=self.config['batch_size'],\n",
        "                is_query=False,\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "            self.index.add_documents(document_ids, documents_embeddings=documents_embeddings)\n",
        "            print(f'Indexed {len(document_ids)} documents')\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f'Error creating index: {e}')\n",
        "            return False\n",
        "\n",
        "    def setup_retriever_generator(self):\n",
        "        \"\"\"Initialize the retriever and generator with prompt template.\"\"\"\n",
        "        try:\n",
        "            self.retriever = retrieve.ColBERT(index=self.index)\n",
        "            self.generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=self.config['max_length'])\n",
        "            prompt_template = '''Using only the provided text, answer the user's question with a concise and accurate response. For questions about specific roles (e.g., CEO, CTO, CFO), return only the full name of the individual in that role. For questions about lists (e.g., products), return all items as a comma-separated list of names only. Exclude any details not directly relevant to the question, such as technical specifications, unless explicitly requested. If the answer is not in the text, respond with 'The answer could not be found in the text.'\n",
        "\n",
        "Text: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:''' \n",
        "            self.prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "            print('Retriever and generator initialized')\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f'Error setting up retriever or generator: {e}')\n",
        "            return False\n",
        "\n",
        "    def query_rag(self, query):\n",
        "        \"\"\"Process a query through the RAG pipeline.\"\"\"\n",
        "        try:\n",
        "            queries = [query]\n",
        "\n",
        "            # Encode Query\n",
        "            query_embedding = self.model.encode(\n",
        "                queries,\n",
        "                batch_size=self.config['batch_size'],\n",
        "                is_query=True,\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "\n",
        "            # Retrieve Top Documents\n",
        "            initial_results = self.retriever.retrieve(queries_embeddings=query_embedding, k=self.config['top_k_initial'])\n",
        "            retrieved_doc_ids = [result['id'] for result in initial_results[0]]\n",
        "            retrieved_documents = [self.document_map[doc_id] for doc_id in retrieved_doc_ids]\n",
        "\n",
        "            # Rerank Documents\n",
        "            reranked_results = rank.rerank(\n",
        "                documents_ids=[retrieved_doc_ids],\n",
        "                queries_embeddings=query_embedding,\n",
        "                documents_embeddings=[self.model.encode(retrieved_documents, is_query=False)]\n",
        "            )\n",
        "\n",
        "            # Get Reranked Documents\n",
        "            reranked_doc_ids = []\n",
        "            if reranked_results and isinstance(reranked_results[0], list):\n",
        "                for result in reranked_results[0]:\n",
        "                    if isinstance(result, dict) and 'id' in result:\n",
        "                        reranked_doc_ids.append(result['id'])\n",
        "                    elif isinstance(result, str):\n",
        "                        reranked_doc_ids.append(result)\n",
        "            else:\n",
        "                reranked_doc_ids = retrieved_doc_ids\n",
        "\n",
        "            reranked_documents = [self.document_map[doc_id] for doc_id in reranked_doc_ids]\n",
        "\n",
        "            # Create Context\n",
        "            context = '\\n'.join(reranked_documents[:3])[:self.config['max_context_length']]\n",
        "            prompt_text = self.prompt.format(context=context, question=query)\n",
        "\n",
        "            # Generate Answer\n",
        "            response = self.generator(prompt_text)[0]['generated_text']\n",
        "            answer = response.strip()\n",
        "\n",
        "            # Post-processing\n",
        "            if ', ' in answer:\n",
        "                items = set(answer.split(', '))\n",
        "                answer = ', '.join(sorted(items)) if items else 'The answer could not be found in the text.'\n",
        "            if any(role in query.lower() for role in ['ceo', 'cto', 'cfo']):\n",
        "                role = query.lower().split('who is')[1].strip().upper()\n",
        "                match = re.search(rf'- ([^,]+), {role}:', context)\n",
        "                if match:\n",
        "                    answer = match.group(1).strip()\n",
        "                else:\n",
        "                    answer = 'The answer could not be found in the text.'\n",
        "            if 'product' in query.lower():\n",
        "                product_names = re.findall(r'- (\\w+):', context)\n",
        "                if product_names:\n",
        "                    answer = ', '.join(sorted(set(product_names)))\n",
        "                else:\n",
        "                    answer = 'The answer could not be found in the text.'\n",
        "\n",
        "            return context, answer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Error processing query: {e}')\n",
        "            return None, 'Error processing query.'\n",
        "\n",
        "    def run(self, pdf_paths):\n",
        "        \"\"\"Run the pipeline for a list of PDFs.\"\"\"\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        for pdf_path in pdf_paths:\n",
        "            texts, ids, doc_map = self.load_and_chunk_pdf(pdf_path)\n",
        "            if texts:\n",
        "                all_document_texts.extend(texts)\n",
        "                all_document_ids.extend(ids)\n",
        "                self.document_map.update(doc_map)\n",
        "\n",
        "        if not all_document_texts:\n",
        "            print('No valid PDFs processed.')\n",
        "            return False\n",
        "\n",
        "        if not self.initialize_models():\n",
        "            return False\n",
        "\n",
        "        if not self.create_index(all_document_texts, all_document_ids):\n",
        "            return False\n",
        "\n",
        "        if not self.setup_retriever_generator():\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "def main(pdf_paths, queries=None):\n",
        "    \"\"\"Orchestrate the RAG pipeline setup and optional querying.\"\"\"\n",
        "    pipeline = RAGPipeline(CONFIG)\n",
        "    if pipeline.run(pdf_paths):\n",
        "        print('Pipeline initialized successfully.')\n",
        "        if queries:\n",
        "            for query in queries:\n",
        "                context, answer = pipeline.query_rag(query)\n",
        "                print(f'Query: {query}\\nAnswer: {answer}\\nContext: {context}\\n')\n",
        "    else:\n",
        "        print('Pipeline initialization failed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 3: Upload or Specify PDFs\n",
        "\n",
        "Upload the PDFs (`QuantumCore_v1.pdf`, `NeoCompute_v3.pdf`) or specify their paths if pre-uploaded to `/data`. This cell prepares the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "# Specify PDF paths or upload\n",
        "pdf_paths = ['/data/QuantumCore_v1.pdf', '/data/NeoCompute_v3.pdf']\n",
        "\n",
        "# Uncomment to upload PDFs manually in Colab\n",
        "# print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v3.pdf):')\n",
        "# uploaded = files.upload()\n",
        "# pdf_paths = list(uploaded.keys())\n",
        "\n",
        "print(f'PDFs to process: {pdf_paths}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 4: Run the Pipeline\n",
        "\n",
        "This cell initializes the RAG pipeline with the specified PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "main(pdf_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq8vM0rRyfNo"
      },
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "This cell allows interactive querying of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq8vM0rYyfNp"
      },
      "outputs": [],
      "source": [
        "pipeline = RAGPipeline(CONFIG)\n",
        "if pipeline.run(pdf_paths):\n",
        "    while True:\n",
        "        query = input('Enter your query (or \"exit\" to stop): ')\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        context, answer = pipeline.query_rag(query)\n",
        "        print(f'\\nQuery: {query}\\nAnswer: {answer}\\nContext: {context}\\n')\n",
        "else:\n",
        "    print('Pipeline initialization failed.')"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0
}