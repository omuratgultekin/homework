{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# RAG Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook implements a Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. It answers queries using two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`, handling questions about leadership, products, and specifications.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI to provide accurate answers from PDFs.\n",
        "- Use lightweight models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`) for CPU compatibility.\n",
        "- Provide an interactive query interface with readable output.\n",
        "- Use Few-Shot Prompting with fictional examples to ensure fairness and generalizability.\n",
        "- Include detailed code comments for improved readability.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded with `PyPDFLoader`, split into chunks (300 chars, 50-char overlap) using `RecursiveCharacterTextSplitter`, and stored with source tracking for company filtering.\n",
        "- **Semantic Layer**: Chunks and queries are embedded with `lightonai/GTE-ModernColBERT-v1`.\n",
        "- **Retrieval**: `retrieve.ColBERT` fetches 15 chunks, reranked to 3 by `rank.rerank`.\n",
        "- **Augmentation**: Top 3 chunks are combined with the query via a Few-Shot `PromptTemplate`.\n",
        "- **Generation**: `google/flan-t5-base` generates answers based on the prompt.\n",
        "\n",
        "## Updates\n",
        "- Added detailed English comments in code cells for better readability.\n",
        "- Fixed JSON parsing error in `prompt_template` for Cursor compatibility.\n",
        "- Implemented Few-Shot Prompting with fictional examples to avoid dataset bias.\n",
        "- Added robust company filtering with fallback for NeoCompute.\n",
        "- Enhanced output formatting with clear sections, suppressed progress bars, and concise progress messages.\n",
        "- Fixed errors for NeoCompute indexing and query handling.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet`.\n",
        "- **Environment**: Google Colab, CPU-friendly.\n",
        "- **Datasets**: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` (quantum computing), `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf` (AI hardware/software).\n",
        "\n",
        "## Instructions\n",
        "1. Cell 1: Install libraries.\n",
        "2. Cell 2: Import libraries.\n",
        "3. Cell 3: Define RAG pipeline with detailed comments.\n",
        "4. Cell 4: Process PDFs with explanatory comments.\n",
        "5. Cell 5: Run interactive queries with formatted output and comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdRrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Installs Python libraries required for the RAG pipeline, ensuring compatibility in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "Imports libraries for the pipeline and suppresses warnings for clean output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for the RAG pipeline\n",
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank  # Pylate modules for embedding, indexing, and retrieval\n",
        "from langchain.document_loaders import PyPDFLoader  # Load PDFs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Split text into chunks\n",
        "from langchain.prompts import PromptTemplate  # Create prompt templates\n",
        "from google.colab import files  # Handle file uploads in Colab\n",
        "import os  # File system operations\n",
        "from transformers import pipeline  # Hugging Face pipeline for text generation\n",
        "import contextlib  # Redirect stderr for clean output\n",
        "import io  # StringIO for stderr redirection\n",
        "from tqdm import tqdm  # Progress bar (disabled)\n",
        "\n",
        "# Disable tqdm progress bars for cleaner output\n",
        "tqdm.__init__ = lambda *args, **kwargs: None\n",
        "\n",
        "# Suppress PDF reader warnings to avoid cluttering output\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "Defines the RAG pipeline functions (`run_rag_pipeline`, `query_rag`) with a Few-Shot prompt using fictional examples. Includes detailed comments to explain each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(pdf_paths):\n",
        "    \"\"\"Initialize the RAG pipeline by processing PDFs, creating embeddings, and setting up models.\"\"\"\n",
        "    try:\n",
        "        # --- Initialize Data Structures ---\n",
        "        # Lists to store document texts and IDs\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        # Dictionary to map document IDs to their text content\n",
        "        document_map = {}\n",
        "        # Counter for generating unique document IDs\n",
        "        current_doc_id = 0\n",
        "        # Dictionary to track the source (QuantumCore/NeoCompute) of each document\n",
        "        document_sources = {}\n",
        "\n",
        "        # --- Process PDFs ---\n",
        "        # Initialize text splitter with 300-char chunks and 50-char overlap\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            # Check if PDF file exists\n",
        "            if not os.path.exists(pdf_path):\n",
        "                print(f'PDF not found: {pdf_path}')\n",
        "                continue\n",
        "            # Load PDF content\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            # Verify that content was loaded\n",
        "            if not documents:\n",
        "                print(f'No content loaded from {pdf_path}')\n",
        "                continue\n",
        "            # Split documents into chunks\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            # Generate unique IDs for chunks\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            # Map IDs to texts\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            # Extract source name (QuantumCore or NeoCompute) from filename\n",
        "            source_name = os.path.basename(pdf_path).split('_')[0]\n",
        "            # Associate IDs with source\n",
        "            document_sources.update({doc_id: source_name for doc_id in document_ids})\n",
        "            # Add texts and IDs to main lists\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            # Update ID counter\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        # --- Validate Document Processing ---\n",
        "        if not all_document_texts:\n",
        "            print('No documents processed.')\n",
        "            return None\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        # --- Set Up Embedding Model ---\n",
        "        # Initialize ColBERT model for embeddings\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        # --- Create Index for Retrieval ---\n",
        "        # Set up Voyager index to store embeddings\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        # --- Generate and Store Embeddings ---\n",
        "        # Encode document texts into embeddings, suppressing stderr for clean output\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            documents_embeddings = model.encode(\n",
        "                all_document_texts,\n",
        "                batch_size=32,\n",
        "                is_query=False,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        # Add embeddings to the index\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        # --- Initialize Retriever ---\n",
        "        # Set up ColBERT retriever for fetching relevant chunks\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        # --- Initialize Generator ---\n",
        "        # Set up FLAN-T5 model for text generation\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        # --- Define Few-Shot Prompt ---\n",
        "        # Create a prompt template with fictional examples to guide answer extraction\n",
        "        prompt_template = r\"\"\"\n",
        "You are an expert assistant answering questions based solely on the provided text. Follow the instructions and examples below to generate concise and accurate responses.\n",
        "\n",
        "**Instructions**:\n",
        "1. For role-based questions (e.g., CEO, CTO), return the full name of the individual in that role.\n",
        "2. For list-based questions (e.g., products, compliance standards), return a comma-separated list of names, sorted alphabetically.\n",
        "3. For detail-based questions (e.g., qubit count), return the exact detail as stated.\n",
        "4. For other questions, provide a brief, relevant answer.\n",
        "5. If the answer is not in the text, return: \"The answer could not be found in the text.\"\n",
        "6. Parse bullet points, sentences, or headings, ignoring irrelevant details unless requested.\n",
        "7. Keep answers concise, avoiding extra explanations.\n",
        "\n",
        "**Examples**:\n",
        "- Text: \"- Jane Smith, CEO: 20 years in tech innovation. - SkyNet: Advanced AI processor.\"\n",
        "  Question: Who is the CEO of Horizon Innovations?\n",
        "  Answer: Jane Smith\n",
        "\n",
        "- Text: \"- CloudPeak: Scalable cloud platform. - SecureVault: Data protection suite.\"\n",
        "  Question: What are the products offered by TechTrend Solutions?\n",
        "  Answer: CloudPeak, SecureVault\n",
        "\n",
        "- Text: \"- PCI DSS certified. - FedRAMP compliant.\"\n",
        "  Question: What compliance standards does DataSafe Inc. follow?\n",
        "  Answer: FedRAMP, PCI DSS\n",
        "\n",
        "- Text: \"- AlphaCore: 100-qubit photonic architecture.\"\n",
        "  Question: What is the qubit count of AlphaCore?\n",
        "  Answer: 100-qubit photonic architecture\n",
        "\n",
        "- Text: \"- Robert Lee, CTO: Expert in cloud systems.\"\n",
        "  Question: Who is the CIO of Horizon Innovations?\n",
        "  Answer: The answer could not be found in the text.\n",
        "\n",
        "**Text**: {context}\n",
        "\n",
        "**Question**: {question}\n",
        "\n",
        "**Answer**:\n",
        "\"\"\"\n",
        "        # Create PromptTemplate object with input variables\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        # --- Return Pipeline Components ---\n",
        "        # Return all components needed for querying\n",
        "        return model, index, retriever, generator, PROMPT, document_map, document_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors during pipeline setup\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query):\n",
        "    \"\"\"Process a user query with retrieval and generation, relying on Few-Shot prompt.\"\"\"\n",
        "    try:\n",
        "        # --- Prepare Query ---\n",
        "        # Convert query to a list for batch processing\n",
        "        queries = [query]\n",
        "\n",
        "        # --- Encode Query ---\n",
        "        print('Encoding query...')\n",
        "        # Encode the query into an embedding, suppressing stderr\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            query_embedding = model.encode(\n",
        "                queries,\n",
        "                batch_size=32,\n",
        "                is_query=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "\n",
        "        # --- Retrieve Documents ---\n",
        "        print('Retrieving documents...')\n",
        "        # Retrieve top 15 relevant document chunks\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            top_k_initial = 15\n",
        "            initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "\n",
        "        # Check if any documents were retrieved\n",
        "        if not initial_results or not initial_results[0]:\n",
        "            print('No documents retrieved.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # Extract document IDs from results\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0] if 'id' in result]\n",
        "        if not retrieved_doc_ids:\n",
        "            print('No document IDs after retrieval.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # --- Filter by Company ---\n",
        "        # Determine company based on query keywords\n",
        "        company = None\n",
        "        filtered_doc_ids = retrieved_doc_ids\n",
        "        if 'quantumcore' in query.lower():\n",
        "            company = 'QuantumCore'\n",
        "        elif 'neocompute' in query.lower():\n",
        "            company = 'NeoCompute'\n",
        "        if company:\n",
        "            # Filter documents by company source\n",
        "            filtered_doc_ids = [doc_id for doc_id in retrieved_doc_ids if document_sources.get(doc_id) == company]\n",
        "            if not filtered_doc_ids:\n",
        "                # Fallback to all documents if no company-specific chunks found\n",
        "                print(f'No documents found for company: {company}. Falling back to all documents.')\n",
        "                filtered_doc_ids = retrieved_doc_ids\n",
        "\n",
        "        # --- Retrieve Document Texts ---\n",
        "        # Get text content for filtered document IDs\n",
        "        retrieved_documents = [document_map.get(doc_id, '') for doc_id in filtered_doc_ids]\n",
        "        retrieved_documents = [doc for doc in retrieved_documents if doc]\n",
        "\n",
        "        # Check if any valid documents remain\n",
        "        if not retrieved_documents:\n",
        "            print('No valid documents after filtering.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # --- Rerank Documents ---\n",
        "        print('Reranking documents...')\n",
        "        # Rerank documents to select top 3 most relevant\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            reranked_results = rank.rerank(\n",
        "                documents_ids=[filtered_doc_ids],\n",
        "                queries_embeddings=query_embedding,\n",
        "                documents_embeddings=[model.encode(retrieved_documents, is_query=False, show_progress_bar=False)]\n",
        "            )\n",
        "\n",
        "        # Extract reranked document IDs\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            # Fallback to top 3 filtered IDs if reranking fails\n",
        "            reranked_doc_ids = filtered_doc_ids[:3]\n",
        "\n",
        "        # Check if reranked IDs are valid\n",
        "        if not reranked_doc_ids:\n",
        "            print('No document IDs after reranking.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # Get texts for reranked documents\n",
        "        reranked_documents = [document_map.get(doc_id, '') for doc_id in reranked_doc_ids]\n",
        "        reranked_documents = [doc for doc in reranked_documents if doc]\n",
        "\n",
        "        # --- Build Context ---\n",
        "        # Combine top 3 documents into context, limiting to 600 characters\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        if not context:\n",
        "            print('No context generated.')\n",
        "            return None, 'No relevant context found.'\n",
        "\n",
        "        # --- Generate Prompt ---\n",
        "        # Format the prompt with context and question\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        # --- Generate Answer ---\n",
        "        print('Generating answer...')\n",
        "        # Use FLAN-T5 to generate the answer\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        # Handle empty or invalid responses\n",
        "        if not answer or answer.lower() == 'none':\n",
        "            answer = 'The answer could not be found in the text.'\n",
        "\n",
        "        # Return context and answer\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors during query processing\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "Initializes the RAG pipeline by processing PDFs, with comments explaining each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "# Define paths to PDF datasets\n",
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "# --- Handle Missing PDFs ---\n",
        "# Check if PDFs exist; prompt for upload if not found\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    # Allow user to upload PDFs in Colab\n",
        "    uploaded = files.upload()\n",
        "    # Update paths to uploaded files\n",
        "    pdf_paths = [f'/content/{name}' for name in uploaded.keys()]\n",
        "\n",
        "# --- Initialize Pipeline ---\n",
        "# Run the RAG pipeline with the PDF paths\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    # Unpack pipeline components if successful\n",
        "    model, index, retriever, generator, PROMPT, document_map, document_sources = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    # Report failure if pipeline initialization fails\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "Provides an interactive query interface with formatted output and detailed comments.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions? → 'Dr. Elena Ruiz'\n",
        "- What are the products offered by NeoCompute Technologies? → 'NeoCloud, NeoSecure'\n",
        "- Who is the CIO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- Who is the CEO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- What is the qubit count of QubitCore? → '50-qubit superconducting architecture'\n",
        "- What compliance standards does NeoCompute follow? → 'ISO/IEC 27001, SOC 2 Type II'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run an interactive query loop with formatted output for user queries.\"\"\"\n",
        "    # --- Display Interface Header ---\n",
        "    # Print a clean header for the query interface\n",
        "    print('=====================================')\n",
        "    print('Interactive RAG Query Interface')\n",
        "    print('=====================================')\n",
        "    print('Enter your query (or type \"exit\" to quit):\n')\n",
        "\n",
        "    # --- Query Loop ---\n",
        "    while True:\n",
        "        # Prompt user for a query\n",
        "        query = input('Query: ').strip()\n",
        "        # Check if user wants to exit\n",
        "        if query.lower() == 'exit':\n",
        "            print('\\nExiting query interface.')\n",
        "            break\n",
        "        # Verify that pipeline is initialized\n",
        "        if not result:\n",
        "            print('\\nError: RAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "\n",
        "        # --- Process Query ---\n",
        "        # Run the query through the RAG pipeline\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query)\n",
        "\n",
        "        # --- Display Results ---\n",
        "        # Print formatted query results\n",
        "        print('\\n====================================')\n",
        "        print(f'Query: {query}')\n",
        "        print('====================================')\n",
        "        print('\\n**Context Retrieved**:\\n')\n",
        "        if context is None:\n",
        "            # Handle case where no context was retrieved\n",
        "            print('    Error: No context retrieved.')\n",
        "        else:\n",
        "            # Format context with indentation for readability\n",
        "            indented_context = context.replace('\\n', '\\n    ')\n",
        "            print(f'    {indented_context}')\n",
        "        print('\\n---')\n",
        "        print('\\n**Answer**:\\n')\n",
        "        # Display the generated answer\n",
        "        print(f'    {answer}')\n",
        "        print('\\n====================================\\n')\n",
        "        print('Enter your next query (or type \"exit\" to quit):\n')\n",
        "\n",
        "# --- Run Interactive Query Interface ---\n",
        "interactive_query()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "nbformat": 4,
    "nbformat_minor": 0
  }
}