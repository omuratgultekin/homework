{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGuZvk_1goLj"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) Pipeline Demo (TAKE_HOME_PROJECT)\n",
        "\n",
        "This Jupyter Notebook implements a minimal Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. The system answers user queries by leveraging content from two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`. It demonstrates versatility in handling varied queries (e.g., leadership roles, product lists, technical specifications) using a lightweight, CPU-friendly setup suitable for Google Colab.\n",
        "\n",
        "## Objective\n",
        "- **Purpose**: Combine retrieval and generative AI to provide accurate, context-grounded answers from PDF content.\n",
        "- **Resource Efficiency**: Use small models (`lightonai/GTE-ModernColBERT-v1` for embeddings, `google/flan-t5-base` for generation) to ensure compatibility with CPU environments.\n",
        "- **Post-Processing(moved to prompt)**: Apply minimal regex-based post-processing for role-based queries (e.g., extracting CEO names) and product queries (e.g., listing product names), with deduplication to ensure clean outputs.\n",
        "- **Interactivity**: Support an interactive query interface for demo purposes, with example queries to showcase functionality.\n",
        "\n",
        "## Architecture\n",
        "The pipeline follows a modular RAG design:\n",
        "- **Knowledge Base**: PDFs are loaded using `PyPDFLoader` and split into chunks (200 characters, 25-character overlap) with `RecursiveCharacterTextSplitter`. Chunks are stored in a dictionary mapping document IDs to text, with source tracking for company-specific filtering.\n",
        "- **Semantic Layer**: Text chunks and queries are embedded into dense vectors using `lightonai/GTE-ModernColBERT-v1` for semantic similarity comparison.\n",
        "- **Retrieval System**: `retrieve.ColBERT` fetches the top 15 relevant chunks based on query embeddings, which are reranked to the top 3 using `rank.rerank` for improved relevance.\n",
        "- **Augmentation**: The top 3 chunks (up to 500 characters) are combined with combined with the query via a Few-Shot PromptTemplate tocreate a contextualized input for the generative model\n",
        "- **Generation**: `google/flan-t5-base` produces concise answers, with post-processing to extract names for role queries (e.g., CEO), list products for product queries, or deduplicate comma-separated lists.\n",
        "- **Fixes Implemented**:\n",
        "\n",
        "  -  Added source tracking to filter chunks by company based on query keywords.\n",
        "  -  Implemented Few-Shot Prompting with fictional examples.\n",
        "\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: Requires `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet` for PDF processing, embedding, retrieval, and generation.\n",
        "- **Environment**: Designed for Google Colab with CPU, ensuring accessibility without GPU requirements.\n",
        "- **Datasets**: Processes `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` (quantum computing company details) and `NeoCompute_Technologies_RAG_Demo_Dataset_v3.pdf` (assumed similar content).\n",
        "\n",
        "## Instructions\n",
        "1. **Cell 1**: Install required Python libraries to set up the environment.\n",
        "2. **Cell 2**: Import libraries and suppress warnings for cleaner output.\n",
        "3. **Cell 3**: Define the RAG pipeline functions (`run_rag_pipeline` and `query_rag`) with improved logic.\n",
        "4. **Cell 4**: Load and process the PDFs, initializing the pipeline with models and indexes.\n",
        "5. **Cell 5**: Run an interactive query interface to test the pipeline with example or custom queries.\n",
        "\n",
        "The pipeline combines chunks from both PDFs into a single knowledge base but filters by company when specified in queries, ensuring relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdKrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "This cell installs the necessary Python libraries for the RAG pipeline. It ensures compatibility in a clean Google Colab environment by installing `pylate` (for ColBERT embeddings and retrieval), `langchain` (for document loading and splitting), `transformers` (for the FLAN-T5 model), `google-colab` (for Colab utilities), and additional dependencies (`langchain-community`, `pypdf`, `hf_xet`) for PDF processing and Hugging Face integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a55cb5-0129-474e-9e81-03163ceb843b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylate\n",
            "  Downloading pylate-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Collecting sentence-transformers==4.0.2 (from pylate)\n",
            "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting datasets>=2.20.0 (from pylate)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from pylate) (1.3.0)\n",
            "Collecting voyager>=2.0.9 (from pylate)\n",
            "  Downloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting sqlitedict>=2.1.0 (from pylate)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from pylate) (2.2.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ujson==5.10.0 (from pylate)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting ninja==1.11.1.4 (from pylate)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting fastkmeans==0.5.0 (from pylate)\n",
            "  Downloading fastkmeans-0.5.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from fastkmeans==0.5.0->pylate) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.0.2->pylate) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.0.2->pylate) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.0.2->pylate) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.0.2->pylate) (4.12.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: google-auth==2.38.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.38.0)\n",
            "Requirement already satisfied: ipykernel==6.17.1 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.17.1)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.5.5)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: tornado==6.4.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.4.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (4.9)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (24.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->google-colab)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (3.1.5)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (5.7.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google-colab) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.1->pylate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.1->pylate) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20.0->pylate) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.20.0->pylate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=2.20.0->pylate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.20.0->pylate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->pylate) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.5->google-colab) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.38.0->google-colab) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->ipyparallel==8.8.0->google-colab) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->fastkmeans==0.5.0->pylate) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->fastkmeans==0.5.0->pylate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->fastkmeans==0.5.0->pylate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->fastkmeans==0.5.0->pylate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->fastkmeans==0.5.0->pylate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->fastkmeans==0.5.0->pylate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->fastkmeans==0.5.0->pylate) (1.3.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==4.0.2->pylate) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==4.0.2->pylate) (3.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.5->google-colab) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.23.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.22)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.8.0)\n",
            "Downloading pylate-1.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.1/134.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastkmeans-0.5.0-py3-none-any.whl (14 kB)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m45.0/56.3 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install core libraries for RAG pipeline (pylate for ColBERT, langchain for document processing, transformers for generation)\n",
        "!pip install pylate langchain transformers google-colab\n",
        "# Install additional dependencies for PDF loading and Hugging Face integration\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B5t0wUQ-09fy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmTg4Mr2lRiL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "This cell imports the required Python libraries for the pipeline and suppresses warnings to ensure cleaner output in Colab. Key libraries include:\n",
        "- `pylate` for ColBERT-based embedding and retrieval (`models`, `indexes`, `retrieve`, `rank`).\n",
        "- `langchain` for PDF loading (`PyPDFLoader`), text splitting (`RecursiveCharacterTextSplitter`), and prompt creation (`PromptTemplate`).\n",
        "- `transformers` for the FLAN-T5 model (`pipeline`).\n",
        "- `google.colab.files` for handling file uploads in Colab.\n",
        "- `os`, `re` for file path handling and regex post-processing.\n",
        "- Warnings from `pypdf` are suppressed to avoid cluttering the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for the RAG pipeline\n",
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank  # Pylate modules for embedding, indexing, and retrieval\n",
        "from langchain.document_loaders import PyPDFLoader  # Load PDFs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Split text into chunks\n",
        "from langchain.prompts import PromptTemplate  # Create prompt templates\n",
        "from google.colab import files  # Handle file uploads in Colab\n",
        "import os  # File system operations\n",
        "from transformers import pipeline  # Hugging Face pipeline for text generation\n",
        "import contextlib  # Redirect stderr for clean output\n",
        "import io  # StringIO for stderr redirection\n",
        "import os\n",
        "\n",
        "\n",
        "# Suppress PDF reader warnings to avoid cluttering output\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33j7ZTP7lyEs"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "This cell defines the core functions of the RAG pipeline:\n",
        "- **`run_rag_pipeline`**: Processes PDFs by loading, chunking, embedding, and indexing them, then initializes the retriever and generator. It  tracks the source PDF for each chunk to enable company-specific filtering. Create a prompt template with fictional examples to guide answer extraction\n",
        "- **`query_rag`**: Handles user queries by encoding them, retrieving and reranking relevant chunks, augmenting the query with context, generating an answer, and applying post-processing. Fixes include:\n",
        "  - **Company Filtering**: Filters chunks by company (QuantumCore or NeoCompute) based on query keywords.\n",
        "  - **Role Extraction**: Uses an improved regex to extract names for roles (e.g., CEO) reliably.\n",
        "  \n",
        "  - **Robust Post-Processing**: Ensures accurate (removed) and fallback to raw generated answers when needed.\n",
        "\n",
        "The pipeline is designed to be robust, handling errors gracefully and providing clear feedback if processing fails. All strings (e.g., prompt template, regex patterns(removed)) are properly escaped to ensure valid JSON."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uf3irb5azqr4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(pdf_paths):\n",
        "    \"\"\"Initialize the RAG pipeline by processing PDFs, creating embeddings, and setting up models.\"\"\"\n",
        "    try:\n",
        "        # --- Initialize Data Structures ---\n",
        "        # Lists to store document texts and IDs\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        # Dictionary to map document IDs to their text content\n",
        "        document_map = {}\n",
        "        # Counter for generating unique document IDs\n",
        "        current_doc_id = 0\n",
        "        # Dictionary to track the source (QuantumCore/NeoCompute) of each document\n",
        "        document_sources = {}\n",
        "\n",
        "        # --- Process PDFs ---\n",
        "        # Initialize text splitter with 300-char chunks and 50-char overlap\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=25)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            # Check if PDF file exists\n",
        "            if not os.path.exists(pdf_path):\n",
        "                print(f'PDF not found: {pdf_path}')\n",
        "                continue\n",
        "            # Load PDF content\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            # Verify that content was loaded\n",
        "            if not documents:\n",
        "                print(f'No content loaded from {pdf_path}')\n",
        "                continue\n",
        "            # Split documents into chunks\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            # Generate unique IDs for chunks\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            # Map IDs to texts\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            # Extract source name (exp:QuantumCore or NeoCompute) from filename\n",
        "            source_name = os.path.basename(pdf_path).split('_')[0]\n",
        "            # Associate IDs with source\n",
        "            document_sources.update({doc_id: source_name for doc_id in document_ids})\n",
        "            # Add texts and IDs to main lists\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            # Update ID counter\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        # --- Validate Document Processing ---\n",
        "        if not all_document_texts:\n",
        "            print('No documents processed.')\n",
        "            return None\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        # --- Set Up Embedding Model ---\n",
        "        # Initialize ColBERT model for embeddings\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        # --- Create Index for Retrieval ---\n",
        "        # Set up Voyager index to store embeddings\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        # --- Generate and Store Embeddings ---\n",
        "        # Encode document texts into embeddings, suppressing stderr for clean output\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            documents_embeddings = model.encode(\n",
        "                all_document_texts,\n",
        "                batch_size=32,\n",
        "                is_query=False,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        # Add embeddings to the index\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        # --- Initialize Retriever ---\n",
        "        # Set up ColBERT retriever for fetching relevant chunks\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        # --- Initialize Generator ---\n",
        "        # Set up FLAN-T5 model for text generation\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        # --- Define Few-Shot Prompt ---\n",
        "        # Create a prompt template with fictional examples to guide answer extraction\n",
        "        prompt_template = r\"\"\"\n",
        "You are an expert assistant answering questions based solely on the provided text. Follow these rules:\n",
        "1. For roles (e.g., CEO), return the full name.\n",
        "2. For lists (e.g., products), return a comma-separated list, sorted alphabetically.\n",
        "3. For details (e.g., specifications), return the exact detail.\n",
        "4. For other questions, provide a brief answer.\n",
        "5. If no answer is found, return: \"The answer could not be found in the text.\"\n",
        "\n",
        "**Examples**:\n",
        "- Text: \"Jane Smith, CEO.\" Question: Who is the CEO? Answer: Jane Smith\n",
        "- Text: \"CloudPeak, SecureVault.\" Question: What products? Answer: CloudPeak, SecureVault\n",
        "- Text: \"AlphaCore: 100 qubits.\" Question: Qubit count? Answer: 100 qubits\n",
        "\n",
        "**Text**: {context}\n",
        "\n",
        "**Question**: {question}\n",
        "\n",
        "**Answer**:\n",
        "\"\"\"\n",
        "        # Create PromptTemplate object with input variables\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        # --- Return Pipeline Components ---\n",
        "        # Return all components needed for querying\n",
        "        return model, index, retriever, generator, PROMPT, document_map, document_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors during pipeline setup\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query):\n",
        "    \"\"\"Process a user query with retrieval and generation, relying on Few-Shot prompt.\"\"\"\n",
        "    try:\n",
        "        # --- Prepare Query ---\n",
        "        # Convert query to a list for batch processing\n",
        "        queries = [query]\n",
        "\n",
        "        # --- Encode Query ---\n",
        "        print('Encoding query...')\n",
        "        # Encode the query into an embedding, suppressing stderr\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            query_embedding = model.encode(\n",
        "                queries,\n",
        "                batch_size=32,\n",
        "                is_query=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "\n",
        "        # --- Retrieve Documents ---\n",
        "        print('Retrieving documents...')\n",
        "        # Retrieve top 15 relevant document chunks\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            top_k_initial = 15\n",
        "            initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "\n",
        "        # Check if any documents were retrieved\n",
        "        if not initial_results or not initial_results[0]:\n",
        "            print('No documents retrieved.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # Extract document IDs from results\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0] if 'id' in result]\n",
        "        if not retrieved_doc_ids:\n",
        "            print('No document IDs after retrieval.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # --- Filter by Company ---\n",
        "        # Determine company based on query keywords\n",
        "        company = None\n",
        "        filtered_doc_ids = retrieved_doc_ids\n",
        "        if 'quantumcore' in query.lower():\n",
        "            company = 'QuantumCore'\n",
        "        elif 'neocompute' in query.lower():\n",
        "            company = 'NeoCompute'\n",
        "        if company:\n",
        "            # Filter documents by company source\n",
        "            filtered_doc_ids = [doc_id for doc_id in retrieved_doc_ids if document_sources.get(doc_id) == company]\n",
        "            if not filtered_doc_ids:\n",
        "                # Fallback to all documents if no company-specific chunks found\n",
        "                print(f'No documents found for company: {company}. Falling back to all documents.')\n",
        "                filtered_doc_ids = retrieved_doc_ids\n",
        "\n",
        "        # --- Retrieve Document Texts ---\n",
        "        # Get text content for filtered document IDs\n",
        "        retrieved_documents = [document_map.get(doc_id, '') for doc_id in filtered_doc_ids]\n",
        "        retrieved_documents = [doc for doc in retrieved_documents if doc]\n",
        "\n",
        "        # Check if any valid documents remain\n",
        "        if not retrieved_documents:\n",
        "            print('No valid documents after filtering.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # --- Rerank Documents ---\n",
        "        print('Reranking documents...')\n",
        "        # Rerank documents to select top 3 most relevant\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            reranked_results = rank.rerank(\n",
        "                documents_ids=[filtered_doc_ids],\n",
        "                queries_embeddings=query_embedding,\n",
        "                documents_embeddings=[model.encode(retrieved_documents, is_query=False, show_progress_bar=False)]\n",
        "            )\n",
        "\n",
        "        # Extract reranked document IDs\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            # Fallback to top 3 filtered IDs if reranking fails\n",
        "            reranked_doc_ids = filtered_doc_ids[:3]\n",
        "\n",
        "        # Check if reranked IDs are valid\n",
        "        if not reranked_doc_ids:\n",
        "            print('No document IDs after reranking.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        # Get texts for reranked documents\n",
        "        reranked_documents = [document_map.get(doc_id, '') for doc_id in reranked_doc_ids]\n",
        "        reranked_documents = [doc for doc in reranked_documents if doc]\n",
        "\n",
        "        # --- Build Context ---\n",
        "        # Combine top 3 documents into context, limiting to 600 characters\n",
        "        max_context_length = 500\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        if not context:\n",
        "            print('No context generated.')\n",
        "            return None, 'No relevant context found.'\n",
        "\n",
        "        # --- Generate Prompt ---\n",
        "        # Format the prompt with context and question\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        # --- Generate Answer ---\n",
        "        print('Generating answer...')\n",
        "        # Use FLAN-T5 to generate the answer\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        # Handle empty or invalid responses\n",
        "        if not answer or answer.lower() == 'none':\n",
        "            answer = 'The answer could not be found in the text.'\n",
        "\n",
        "        # Return context and answer\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors during query processing\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDvYFQbjoh5-"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "This cell specifies the paths to the PDF datasets (`QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`) and initializes the RAG pipeline by calling `run_rag_pipeline`. If running locally, ensure the PDFs are in the `/data` directory. In Google Colab, the cell checks for missing files and prompts the user to upload them. The pipeline is initialized with the ColBERT model, Voyager index, retriever, FLAN-T5 generator, prompt template, document map, and source tracking for company-specific filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "# Define paths to PDF datasets\n",
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "# --- Handle Missing PDFs ---\n",
        "# Check if PDFs exist; prompt for upload if not found\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    # Allow user to upload PDFs in Colab\n",
        "    uploaded = files.upload()\n",
        "    # Update paths to uploaded files\n",
        "    pdf_paths = [f'/content/{name}' for name in uploaded.keys()]\n",
        "\n",
        "# --- Initialize Pipeline ---\n",
        "# Run the RAG pipeline with the PDF paths\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    # Unpack pipeline components if successful\n",
        "    model, index, retriever, generator, PROMPT, document_map, document_sources = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    # Report failure if pipeline initialization fails\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4VyrvEjqp0P"
      },
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "This cell provides an interactive interface to query the RAG system. Users can enter custom queries or use provided examples. The system retrieves relevant chunks, generates an answer, and displays both the context and response. Example queries test various aspects of the pipeline:\n",
        "- Role queries (e.g., CEO name) use regex to extract full names.\n",
        "- Product queries list product names, with fixes to exclude non-products.\n",
        "- Specification queries (e.g., qubit count) extract specific details.\n",
        "- Compliance queries return lists of standards, deduplicated and filtered.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions?\n",
        "- What are the products offered by NeoCompute Technologies?\n",
        "- What is the qubit count of QubitCore? → Expected: '\n",
        "- What compliance standards does NeoCompute follow?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGknEq3cJRYQ"
      },
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run an interactive query loop with formatted output for user queries.\"\"\"\n",
        "    # --- Display Interface Header ---\n",
        "    # Print a clean header for the query interface\n",
        "    print('=====================================')\n",
        "    print('Interactive RAG Query Interface')\n",
        "    print('=====================================')\n",
        "    print('Enter your query (or type \"exit\" to quit):')\n",
        "\n",
        "    # --- Query Loop ---\n",
        "    while True:\n",
        "        # Prompt user for a query\n",
        "        query = input('Query: ').strip()\n",
        "        # Check if user wants to exit\n",
        "        if query.lower() == 'exit':\n",
        "            print('\\nExiting query interface.')\n",
        "            break\n",
        "        # Verify that pipeline is initialized\n",
        "        if not result:\n",
        "            print('\\nError: RAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "\n",
        "        # --- Process Query ---\n",
        "        # Run the query through the RAG pipeline\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query)\n",
        "\n",
        "        # --- Display Results ---\n",
        "        # Print formatted query results\n",
        "        print('\\n====================================')\n",
        "        print(f'Query: {query}')\n",
        "        print('====================================')\n",
        "        print('\\n**Context Retrieved**:\\n')\n",
        "        if context is None:\n",
        "            # Handle case where no context was retrieved\n",
        "            print('    Error: No context retrieved.')\n",
        "        else:\n",
        "            # Format context with indentation for readability\n",
        "            indented_context = context.replace('\\n', '\\n    ')\n",
        "            print(f'    {indented_context}')\n",
        "        print('\\n---')\n",
        "        print('\\n**Answer**:\\n')\n",
        "        # Display the generated answer\n",
        "        print(f'    {answer}')\n",
        "        print('\\n====================================\\n')\n",
        "        print('Enter your next query (or type \"exit\" to quit):')\n",
        "\n",
        "# --- Run Interactive Query Interface ---\n",
        "#interactive_query()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CBvsus9i2Zmw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2hNTO3uc-vG"
      },
      "source": [
        "## Cell 6: Interactive Querying\n",
        "\n",
        "\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions? → 'Dr. Elena Ruiz'\n",
        "- What are the products offered by NeoCompute Technologies? → 'NeoCloud, NeoSecure'\n",
        "- Who is the CIO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- Who is the CEO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- What is the qubit count of QubitCore? → '50-qubit superconducting architecture'\n",
        "- What compliance standards does NeoCompute follow? → 'ISO/IEC 27001, SOC 2 Type II'"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJs2Jli62fog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run Interactive Query Interface ---\n",
        "interactive_query()"
      ],
      "metadata": {
        "id": "X1JKcOJZduPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}