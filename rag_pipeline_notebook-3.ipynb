{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook implements a minimal Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. The system answers user queries based on two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`, demonstrating versatility for varied queries.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI to answer queries grounded in PDF content.\n",
        "- Use small, CPU-friendly models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`) to minimize resource usage.\n",
        "- Minimize post-processing with regex for roles/products and deduplication.\n",
        "- Support interactive querying for demo purposes, with example queries provided.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded using `PyPDFLoader`, split into chunks with `RecursiveCharacterTextSplitter` (300 characters, 50 overlap), and stored in memory as a dictionary mapping document IDs to text chunks.\n",
        "- **Semantic Layer**: Chunks and queries are embedded using `lightonai/GTE-ModernColBERT-v1` for semantic comparison, producing dense vector representations.\n",
        "- **Retrieval System**: `retrieve.ColBERT` fetches the top 15 chunks based on query embeddings, which are then reranked to the top 3 using `rank.rerank` for relevance.\n",
        "- **Augmentation**: The top 3 chunks (up to 600 characters total) are combined with the query using a `PromptTemplate` to create a contextualized input for generation.\n",
        "- **Generation**: `google/flan-t5-base` generates concise answers, with regex-based post-processing for role-based queries (e.g., CEO, CTO) to extract names and product queries to list names, ensuring deduplication via sets.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet`.\n",
        "- **Environment**: Designed for Google Colab with CPU, ensuring accessibility.\n",
        "- **Datasets**: Processes `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`.\n",
        "\n",
        "## Instructions\n",
        "1. Run Cell 1 to install dependencies.\n",
        "2. Run Cell 2 to import libraries.\n",
        "3. Run Cell 3 to define the RAG pipeline functions.\n",
        "4. Run Cell 4 to process the PDFs and initialize the pipeline.\n",
        "5. Run Cell 5 to interactively query the system with example queries or custom inputs.\n",
        "\n",
        "The pipeline processes both PDFs sequentially, combining their chunks into a single knowledge base for querying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdKrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Install required libraries for the RAG pipeline. This ensures compatibility in a clean Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "Import necessary libraries and suppress warnings for cleaner output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterWarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterWarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "Define the RAG pipeline functions to process PDFs and handle queries:\n",
        "- `run_rag_pipeline`: Loads and chunks PDFs, embeds chunks using ColBERT, indexes embeddings in Voyager, and initializes the retriever and generator.\n",
        "- `query_rag`: Encodes queries, retrieves and reranks relevant chunks, augments the query with context, and generates answers with post-processing for role and product queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(pdf_paths):\n",
        "    try:\n",
        "        # Initialize combined document storage\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        document_map = {}\n",
        "        current_doc_id = 0\n",
        "\n",
        "        # Load and Chunk PDFs\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        # Load ColBERT Model\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        # Initialize Voyager Index\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        # Create and Index Embeddings\n",
        "        documents_embeddings = model.encode(\n",
        "            all_document_texts,\n",
        "            batch_size=32,\n",
        "            is_query=False,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        # Initialize Retriever\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        # Initialize FLAN-T5 Generator\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        # Define Prompt Template\n",
        "        prompt_template = '''Using only the provided text, answer the user's question with a concise and accurate response. For questions about specific roles (e.g., CEO, CTO, CFO), return only the full name of the individual in that role. For questions about lists (e.g., products), return all items as a comma-separated list of names only. Exclude any details not directly relevant to the question, such as technical specifications, unless explicitly requested. If the answer is not in the text, respond with 'The answer could not be found in the text.'\n",
        "\n",
        "Text: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:''' \n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "        return model, index, retriever, generator, PROMPT, document_map\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, query):\n",
        "    try:\n",
        "        queries = [query]\n",
        "\n",
        "        # Encode Query\n",
        "        query_embedding = model.encode(\n",
        "            queries,\n",
        "            batch_size=32,\n",
        "            is_query=True,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "        # Retrieve Top Documents\n",
        "        top_k_initial = 15\n",
        "        initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0]]\n",
        "        retrieved_documents = [document_map[doc_id] for doc_id in retrieved_doc_ids]\n",
        "\n",
        "        # Rerank Documents\n",
        "        reranked_results = rank.rerank(\n",
        "            documents_ids=[retrieved_doc_ids],\n",
        "            queries_embeddings=query_embedding,\n",
        "            documents_embeddings=[model.encode(retrieved_documents, is_query=False)]\n",
        "        )\n",
        "\n",
        "        # Get Reranked Documents\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            reranked_doc_ids = retrieved_doc_ids\n",
        "\n",
        "        reranked_documents = [document_map[doc_id] for doc_id in reranked_doc_ids]\n",
        "\n",
        "        # Create Context\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        # Generate Answer\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        # Post-processing\n",
        "        if ', ' in answer:\n",
        "            items = set(answer.split(', '))\n",
        "            answer = ', '.join(sorted(items)) if items else 'The answer could not be found in the text.'\n",
        "        if any(role in query.lower() for role in ['ceo', 'cto', 'cfo']):\n",
        "            role = query.lower().split('who is')[1].strip().upper()\n",
        "            match = re.search(rf'- ([^,]+), {role}:', context)\n",
        "            if match:\n",
        "                answer = match.group(1).strip()\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        if 'product' in query.lower():\n",
        "            product_names = re.findall(r'- (\\w+):', context)\n",
        "            if product_names:\n",
        "                answer = ', '.join(sorted(set(product_names)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "Specify the paths to the PDFs (`QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf`, `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`) and initialize the RAG pipeline. If running locally, ensure the PDFs are in the specified directory (e.g., `/data`). In Colab, upload the PDFs manually when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "# Specify PDF paths (modify as needed for your environment)\n",
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "# Check if running in Colab and prompt for upload if files are missing\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "\n",
        "# Run the RAG pipeline\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    model, index, retriever, generator, PROMPT, document_map = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "Run this cell to interactively query the RAG system. Example queries are provided based on the PDFs. Enter a custom query or use one of the examples below. The system will retrieve relevant chunks, generate an answer, and display both the context and response.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions?\n",
        "- What are the products offered by NeoCompute Technologies?\n",
        "- What is the qubit count of QubitCore?\n",
        "- What compliance standards does NeoCompute follow?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    print('Enter your query (or type \"exit\" to quit):')\n",
        "    while True:\n",
        "        query = input('Query: ')\n",
        "        if query.lower() == 'exit':\n",
        "            print('Exiting query interface.')\n",
        "            break\n",
        "        if not result:\n",
        "            print('RAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, query)\n",
        "        print('\\n**Context Retrieved**:\\n', context)\n",
        "        print('\\n**Answer**:\\n', answer, '\\n')\n",
        "\n",
        "# Run interactive query interface\n",
        "interactive_query()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
