{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation (RAG) Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook demonstrates a minimal Retrieval-Augmented Generation (RAG) pipeline designed for a 3-5 hour take-home project interview. The system answers user queries based on two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v3.pdf`, showcasing versatility for varied queries.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI to answer queries grounded in PDF content.\n",
        "- Use small, CPU-friendly models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`).\n",
        "- Minimize post-processing with regex for roles/products and deduplication.\n",
        "- Support interactive querying for demo purposes.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded (`PyPDFLoader`), split into chunks (`RecursiveCharacterTextSplitter`), and stored in memory.\n",
        "- **Semantic Layer**: Chunks and queries are embedded using `lightonai/GTE-ModernColBERT-v1` for semantic comparison.\n",
        "- **Retrieval System**: `retrieve.ColBERT` fetches top 15 chunks, reranked to top 3 (`rank.rerank`).\n",
        "- **Augmentation**: Retrieved chunks (600-char limit) are combined with the query via `PromptTemplate`.\n",
        "- **Generation**: `google/flan-t5-base` generates answers, post-processed with regex and deduplication.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google.colab`, `re`.\n",
        "- **Environment**: Google Colab with CPU.\n",
        "- **Datasets**: PDFs in `/data` folder or uploaded manually.\n",
        "\n",
        "Run the cells below to set up and test the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Install required libraries for the RAG pipeline. This ensures the notebook runs in a clean Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Import Libraries and Define RAG Pipeline\n",
        "\n",
        "This cell imports libraries, suppresses warnings, and defines the RAG pipeline. The pipeline:\n",
        "- Loads and chunks PDFs.\n",
        "- Embeds chunks using ColBERT.\n",
        "- Indexes embeddings in Voyager.\n",
        "- Retrieves and reranks chunks for queries.\n",
        "- Augments queries with context.\n",
        "- Generates answers with `flan-t5-base`, applying minimal post-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning, module='pypdf._reader')\n",
        "\n",
        "def run_rag_pipeline(pdf_path):\n",
        "    try:\n",
        "        # Load and Chunk PDF\n",
        "        print(f'Processing PDF: {pdf_path}')\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        chunks = text_splitter.split_documents(documents)\n",
        "        document_texts = [chunk.page_content for chunk in chunks]\n",
        "        document_ids = [str(i) for i in range(len(document_texts))]\n",
        "        document_map = dict(zip(document_ids, document_texts))\n",
        "        print(f'Created {len(document_texts)} chunks')\n",
        "\n",
        "        # Load ColBERT Model\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        # Initialize Voyager Index\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        # Create and Index Embeddings\n",
        "        documents_embeddings = model.encode(\n",
        "            document_texts,\n",
        "            batch_size=32,\n",
        "            is_query=False,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        index.add_documents(document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        # Initialize Retriever\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        # Initialize FLAN-T5 Generator\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        # Define Prompt Template\n",
        "        prompt_template = '''Using only the provided text, answer the user's question with a concise and accurate response. For questions about specific roles (e.g., CEO, CTO, CFO), return only the full name of the individual in that role. For questions about lists (e.g., products), return all items as a comma-separated list of names only. Exclude any details not directly relevant to the question, such as technical specifications, unless explicitly requested. If the answer is not in the text, respond with 'The answer could not be found in the text.'\n",
        "\n",
        "Text: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:''' \n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "        return model, index, retriever, generator, PROMPT, document_map\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDF: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, query):\n",
        "    try:\n",
        "        queries = [query]\n",
        "\n",
        "        # Encode Query\n",
        "        query_embedding = model.encode(\n",
        "            queries,\n",
        "            batch_size=32,\n",
        "            is_query=True,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "        # Retrieve Top Documents\n",
        "        top_k_initial = 15\n",
        "        initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0]]\n",
        "        retrieved_documents = [document_map[doc_id] for doc_id in retrieved_doc_ids]\n",
        "\n",
        "        # Rerank Documents\n",
        "        reranked_results = rank.rerank(\n",
        "            documents_ids=[retrieved_doc_ids],\n",
        "            queries_embeddings=query_embedding,\n",
        "            documents_embeddings=[model.encode(retrieved_documents, is_query=False)]\n",
        "        )\n",
        "\n",
        "        # Get Reranked Documents\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            reranked_doc_ids = retrieved_doc_ids\n",
        "\n",
        "        reranked_documents = [document_map[doc_id] for doc_id in reranked_doc_ids]\n",
        "\n",
        "        # Create Context\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        # Generate Answer\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        # Post-processing\n",
        "        if ', ' in answer:\n",
        "            items = set(answer.split(', '))\n",
        "            answer = ', '.join(sorted(items)) if items else 'The answer could not be found in the text.'\n",
        "        if any(role in query.lower() for role in ['ceo', 'cto', 'cfo']):\n",
        "            role = query.lower().split('who is')[1].strip().upper()\n",
        "            match = re.search(rf'- ([^,]+), {role}:', context)\n",
        "            if match:\n",
        "                answer = match.group(1).strip()\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        if 'product' in query.lower():\n",
        "            product_names = re.findall(r'- (\\w+):', context)\n",
        "            if product_names:\n",
        "                answer = ', '.join(sorted(set(product_names)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Upload or Specify PDFs\n",
        "\n",
        "Upload the PDFs (`QuantumCore_v1.pdf`, `NeoCompute_v3.pdf`) or specify their paths if pre-uploaded to `/data`. This cell prepares the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Upload PDFs\n",
        "print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v3.pdf):')\n",
        "uploaded = files.upload()\n",
        "pdf_paths = list(uploaded.keys())\n",
        "\n",
        "# Option 2: Specify pre-uploaded PDFs\n",
        "# pdf_paths = ['/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf', '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v3.pdf']\n",
        "\n",
        "if not pdf_paths:\n",
        "    print('No PDFs provided. Please upload or specify paths.')\n",
        "else:\n",
        "    print(f'PDFs to process: {pdf_paths}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Process PDFs and Initialize Pipeline\n",
        "\n",
        "Process each PDF to create chunks, embeddings, and index. Initialize the RAG components for querying. This cell sets up the pipeline for both PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipelines = {}\n",
        "for pdf_path in pdf_paths:\n",
        "    result = run_rag_pipeline(pdf_path)\n",
        "    if result:\n",
        "        model, index, retriever, generator, PROMPT, document_map = result\n",
        "        pipelines[pdf_path] = {\n",
        "            'model': model,\n",
        "            'index': index,\n",
        "            'retriever': retriever,\n",
        "            'generator': generator,\n",
        "            'PROMPT': PROMPT,\n",
        "            'document_map': document_map\n",
        "        }\n",
        "        print(f'Pipeline initialized for {pdf_path}')\n",
        "    else:\n",
        "        print(f'Failed to initialize pipeline for {pdf_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Test Sample Queries for QuantumCore_v1\n",
        "\n",
        "Run sample queries for `QuantumCore_v1.pdf` to demonstrate the pipeline. Queries test varied scenarios (roles, lists, descriptive answers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantumcore_pdf = next((p for p in pdf_paths if 'QuantumCore' in p), None)\n",
        "if quantumcore_pdf and quantumcore_pdf in pipelines:\n",
        "    print(f'\\nTesting sample queries for {quantumcore_pdf}')\n",
        "    pipeline = pipelines[quantumcore_pdf]\n",
        "    queries = [\n",
        "        'what is the company name',\n",
        "        'who is CTO',\n",
        "        'What are the products offered by the company',\n",
        "        'What is the company’s case study about'\n",
        "    ]\n",
        "    for query in queries:\n",
        "        context, answer = query_rag(\n",
        "            pipeline['model'],\n",
        "            pipeline['index'],\n",
        "            pipeline['retriever'],\n",
        "            pipeline['generator'],\n",
        "            pipeline['PROMPT'],\n",
        "            pipeline['document_map'],\n",
        "            query\n",
        "        )\n",
        "        print(f'\\nQuestion: {query!r}')\n",
        "        print(f'Context: {context}')\n",
        "        print(f'Answer: {answer}')\n",
        "else:\n",
        "    print('QuantumCore PDF not found or pipeline not initialized.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Test Sample Queries for NeoCompute_v3\n",
        "\n",
        "Run sample queries for `NeoCompute_v3.pdf` to demonstrate versatility across datasets. Queries test roles, lists, and contact info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neocompute_pdf = next((p for p in pdf_paths if 'NeoCompute' in p), None)\n",
        "if neocompute_pdf and neocompute_pdf in pipelines:\n",
        "    print(f'\\nTesting sample queries for {neocompute_pdf}')\n",
        "    pipeline = pipelines[neocompute_pdf]\n",
        "    queries = [\n",
        "        'what is the company vision',\n",
        "        'what are the products',\n",
        "        'What is the company’s contact email',\n",
        "        'who is CTO'\n",
        "    ]\n",
        "    for query in queries:\n",
        "        context, answer = query_rag(\n",
        "            pipeline['model'],\n",
        "            pipeline['index'],\n",
        "            pipeline['retriever'],\n",
        "            pipeline['generator'],\n",
        "            pipeline['PROMPT'],\n",
        "            pipeline['document_map'],\n",
        "            query\n",
        "        )\n",
        "        print(f'\\nQuestion: {query!r}')\n",
        "        print(f'Context: {context}')\n",
        "        print(f'Answer: {answer}')\n",
        "else:\n",
        "    print('NeoCompute PDF not found or pipeline not initialized.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Interactive Query Interface\n",
        "\n",
        "Allow interactive querying for either PDF. Select a PDF and enter queries to test the pipeline live during the demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nInteractive Query Interface')\n",
        "print('Available PDFs:', list(pipelines.keys()))\n",
        "pdf_choice = input('Select a PDF (enter full path or partial name): ')\n",
        "selected_pdf = next((p for p in pipelines if pdf_choice in p), None)\n",
        "\n",
        "if selected_pdf:\n",
        "    pipeline = pipelines[selected_pdf]\n",
        "    while True:\n",
        "        query = input('Enter your question (or type \"exit\" to quit): ')\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        if not query.strip():\n",
        "            print('Empty query. Please enter a valid question.')\n",
        "            continue\n",
        "        context, answer = query_rag(\n",
        "            pipeline['model'],\n",
        "            pipeline['index'],\n",
        "            pipeline['retriever'],\n",
        "            pipeline['generator'],\n",
        "            pipeline['PROMPT'],\n",
        "            pipeline['document_map'],\n",
        "            query\n",
        "        )\n",
        "        print(f'\\nQuestion: {query!r}')\n",
        "        print(f'Context: {context}')\n",
        "        print(f'Answer: {answer}')\n",
        "else:\n",
        "    print('Invalid PDF selection.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Observations\n",
        "\n",
        "### QuantumCore_v1 Results\n",
        "- **Accuracy**: 5/10 correct (name, workers, CTO, CEO, headquarters).\n",
        "- **Issues**: Incomplete products (`QubitCore` only), truncated case study, missed compliance standards, incorrect partners, and goal/vision confusion.\n",
        "- **Fixes**: Regex for products/roles ensures accuracy for those queries.\n",
        "\n",
        "### NeoCompute_v3 Results\n",
        "- **Accuracy**: 2/9 correct (vision, employees).\n",
        "- **Issues**: Incorrect CTO, incomplete products, garbled mission, missed NeoCloud specs.\n",
        "- **Fixes**: Regex improves roles/products; retrieval misses persist.\n",
        "\n",
        "### Challenges\n",
        "- `flan-t5-base` struggles with lists, complex queries, and prompt adherence.\n",
        "- Retrieval misses (e.g., partners, specs) due to ranking.\n",
        "- Truncation for case studies (`max_length=300`).\n",
        "\n",
        "### Improvements\n",
        "- Use `flan-t5-large` for better generation.\n",
        "- Increase `top_k_initial` to 20 for better retrieval.\n",
        "- Add regex for specifications to handle queries like QubitCore specs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This RAG pipeline meets the interview requirements by:\n",
        "- Using small, CPU-based models.\n",
        "- Supporting varied queries across two PDFs.\n",
        "- Minimizing post-processing with regex and deduplication.\n",
        "- Providing an interactive demo.\n",
        "\n",
        "For production, consider larger models or additional regex for specifications. Test the interactive interface above for live demo purposes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}