{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# RAG Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook implements a Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. It answers queries using two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`, handling questions about leadership, products, and specifications.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI for accurate answers from PDFs.\n",
        "- Use lightweight models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`) for CPU compatibility.\n",
        "- Provide an interactive query interface with readable output.\n",
        "- Use Few-Shot Prompting with fictional examples to ensure fairness and generalizability.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded with `PyPDFLoader`, split into chunks (300 chars, 50-char overlap) using `RecursiveCharacterTextSplitter`, and stored with source tracking for company filtering.\n",
        "- **Semantic Layer**: Chunks and queries are embedded with `lightonai/GTE-ModernColBERT-v1`.\n",
        "- **Retrieval**: `retrieve.ColBERT` fetches 15 chunks, reranked to 3 by `rank.rerank`.\n",
        "- **Augmentation**: Top 3 chunks are combined with the query via a Few-Shot `PromptTemplate`.\n",
        "- **Generation**: `google/flan-t5-base` generates answers based on the prompt.\n",
        "\n",
        "## Updates\n",
        "- Fixed JSON parsing error in `prompt_template` for Cursor compatibility.\n",
        "- Implemented Few-Shot Prompting with fictional examples to avoid dataset bias.\n",
        "- Added robust company filtering with fallback for NeoCompute.\n",
        "- Enhanced output formatting with clear sections, suppressed progress bars, and concise progress messages.\n",
        "- Fixed errors for NeoCompute indexing and query handling.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet`.\n",
        "- **Environment**: Google Colab, CPU-friendly.\n",
        "- **Datasets**: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` (quantum computing), `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf` (AI hardware/software).\n",
        "\n",
        "## Instructions\n",
        "1. Cell 1: Install libraries.\n",
        "2. Cell 2: Import libraries.\n",
        "3. Cell 3: Define RAG pipeline.\n",
        "4. Cell 4: Process PDFs.\n",
        "5. Cell 5: Run interactive queries with formatted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdRrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Installs Python libraries for the RAG pipeline, ensuring compatibility in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "Imports libraries for the pipeline and suppresses warnings for clean output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import contextlib\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.__init__ = lambda *args, **kwargs: None  # Disable tqdm progress bars\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "Defines the RAG pipeline with a Few-Shot prompt using fictional examples, properly escaped for JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(pdf_paths):\n",
        "    \"\"\"Initialize the RAG pipeline by processing PDFs, creating embeddings, and setting up models.\"\"\"\n",
        "    try:\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        document_map = {}\n",
        "        current_doc_id = 0\n",
        "        document_sources = {}\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            if not os.path.exists(pdf_path):\n",
        "                print(f'PDF not found: {pdf_path}')\n",
        "                continue\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            if not documents:\n",
        "                print(f'No content loaded from {pdf_path}')\n",
        "                continue\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            source_name = os.path.basename(pdf_path).split('_')[0]\n",
        "            document_sources.update({doc_id: source_name for doc_id in document_ids})\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        if not all_document_texts:\n",
        "            print('No documents processed.')\n",
        "            return None\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            documents_embeddings = model.encode(\n",
        "                all_document_texts,\n",
        "                batch_size=32,\n",
        "                is_query=False,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        prompt_template = r\"\"\"\n",
        "You are an expert assistant answering questions based solely on the provided text. Follow the instructions and examples below to generate concise and accurate responses.\n",
        "\n",
        "**Instructions**:\n",
        "1. For role-based questions (e.g., CEO, CTO), return the full name of the individual in that role.\n",
        "2. For list-based questions (e.g., products, compliance standards), return a comma-separated list of names, sorted alphabetically.\n",
        "3. For detail-based questions (e.g., qubit count), return the exact detail as stated.\n",
        "4. For other questions, provide a brief, relevant answer.\n",
        "5. If the answer is not in the text, return: \"The answer could not be found in the text.\"\n",
        "6. Parse bullet points, sentences, or headings, ignoring irrelevant details unless requested.\n",
        "7. Keep answers concise, avoiding extra explanations.\n",
        "\n",
        "**Examples**:\n",
        "- Text: \"- Jane Smith, CEO: 20 years in tech innovation. - SkyNet: Advanced AI processor.\"\n",
        "  Question: Who is the CEO of Horizon Innovations?\n",
        "  Answer: Jane Smith\n",
        "\n",
        "- Text: \"- CloudPeak: Scalable cloud platform. - SecureVault: Data protection suite.\"\n",
        "  Question: What are the products offered by TechTrend Solutions?\n",
        "  Answer: CloudPeak, SecureVault\n",
        "\n",
        "- Text: \"- PCI DSS certified. - FedRAMP compliant.\"\n",
        "  Question: What compliance standards does DataSafe Inc. follow?\n",
        "  Answer: FedRAMP, PCI DSS\n",
        "\n",
        "- Text: \"- AlphaCore: 100-qubit photonic architecture.\"\n",
        "  Question: What is the qubit count of AlphaCore?\n",
        "  Answer: 100-qubit photonic architecture\n",
        "\n",
        "- Text: \"- Robert Lee, CTO: Expert in cloud systems.\"\n",
        "  Question: Who is the CIO of Horizon Innovations?\n",
        "  Answer: The answer could not be found in the text.\n",
        "\n",
        "**Text**: {context}\n",
        "\n",
        "**Question**: {question}\n",
        "\n",
        "**Answer**:\n",
        "\"\"\"\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        return model, index, retriever, generator, PROMPT, document_map, document_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query):\n",
        "    \"\"\"Process a user query with retrieval and generation, relying on Few-Shot prompt.\"\"\"\n",
        "    try:\n",
        "        queries = [query]\n",
        "\n",
        "        print('Encoding query...')\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            query_embedding = model.encode(\n",
        "                queries,\n",
        "                batch_size=32,\n",
        "                is_query=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "\n",
        "        print('Retrieving documents...')\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            top_k_initial = 15\n",
        "            initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "\n",
        "        if not initial_results or not initial_results[0]:\n",
        "            print('No documents retrieved.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0] if 'id' in result]\n",
        "        if not retrieved_doc_ids:\n",
        "            print('No document IDs after retrieval.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        company = None\n",
        "        filtered_doc_ids = retrieved_doc_ids\n",
        "        if 'quantumcore' in query.lower():\n",
        "            company = 'QuantumCore'\n",
        "        elif 'neocompute' in query.lower():\n",
        "            company = 'NeoCompute'\n",
        "        if company:\n",
        "            filtered_doc_ids = [doc_id for doc_id in retrieved_doc_ids if document_sources.get(doc_id) == company]\n",
        "            if not filtered_doc_ids:\n",
        "                print(f'No documents found for company: {company}. Falling back to all documents.')\n",
        "                filtered_doc_ids = retrieved_doc_ids  # Fallback to unfiltered\n",
        "\n",
        "        retrieved_documents = [document_map.get(doc_id, '') for doc_id in filtered_doc_ids]\n",
        "        retrieved_documents = [doc for doc in retrieved_documents if doc]\n",
        "\n",
        "        if not retrieved_documents:\n",
        "            print('No valid documents after filtering.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        print('Reranking documents...')\n",
        "        with contextlib.redirect_stderr(io.StringIO()):\n",
        "            reranked_results = rank.rerank(\n",
        "                documents_ids=[filtered_doc_ids],\n",
        "                queries_embeddings=query_embedding,\n",
        "                documents_embeddings=[model.encode(retrieved_documents, is_query=False, show_progress_bar=False)]\n",
        "            )\n",
        "\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            reranked_doc_ids = filtered_doc_ids[:3]\n",
        "\n",
        "        if not reranked_doc_ids:\n",
        "            print('No document IDs after reranking.')\n",
        "            return None, 'No relevant documents found.'\n",
        "\n",
        "        reranked_documents = [document_map.get(doc_id, '') for doc_id in reranked_doc_ids]\n",
        "        reranked_documents = [doc for doc in reranked_documents if doc]\n",
        "\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        if not context:\n",
        "            print('No context generated.')\n",
        "            return None, 'No relevant context found.'\n",
        "\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        print('Generating answer...')\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        if not answer or answer.lower() == 'none':\n",
        "            answer = 'The answer could not be found in the text.'\n",
        "\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "Initializes the RAG pipeline with robust PDF handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = [f'/content/{name}' for name in uploaded.keys()]\n",
        "\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    model, index, retriever, generator, PROMPT, document_map, document_sources = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "Interactive query interface with formatted output and error handling.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions? → 'Dr. Elena Ruiz'\n",
        "- What are the products offered by NeoCompute Technologies? → 'NeoCloud, NeoSecure'\n",
        "- Who is the CIO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- Who is the CEO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- What is the qubit count of QubitCore? → '50-qubit superconducting architecture'\n",
        "- What compliance standards does NeoCompute follow? → 'ISO/IEC 27001, SOC 2 Type II'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run an interactive query loop with formatted output.\"\"\"\n",
        "    print('=====================================')\n",
        "    print('Interactive RAG Query Interface')\n",
        "    print('=====================================')\n",
        "    print('Enter your query (or type \"exit\" to quit):\n",
        "')\n",
        "    while True:\n",
        "        query = input('Query: ').strip()\n",
        "        if query.lower() == 'exit':\n",
        "            print('\\nExiting query interface.')\n",
        "            break\n",
        "        if not result:\n",
        "            print('\\nError: RAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query)\n",
        "        print('\\n====================================')\n",
        "        print(f'Query: {query}')\n",
        "        print('====================================')\n",
        "        print('\\n**Context Retrieved**:\\n')\n",
        "        if context is None:\n",
        "            print('    Error: No context retrieved.')\n",
        "        else:\n",
        "            indented_context = context.replace('\\n', '\\n    ')\n",
        "            print(f'    {indented_context}')\n",
        "        print('\\n---')\n",
        "        print('\\n**Answer**:\\n')\n",
        "        print(f'    {answer}')\n",
        "        print('\\n====================================\\n')\n",
        "        print('Enter your next query (or type \"exit\" to quit):\n",
        "')\n",
        "\n",
        "interactive_query()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "nbformat": 4,
    "nbformat_minor": 0
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
