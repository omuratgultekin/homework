{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0U4apZkrBcJ"
      },
      "source": [
        "# RAG Pipeline Demo\n",
        "\n",
        "This Jupyter Notebook implements a Retrieval-Augmented Generation (RAG) pipeline for a take-home project interview. It answers queries using two PDF datasets: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` and `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`, handling questions about leadership, products, and specifications.\n",
        "\n",
        "## Objective\n",
        "- Combine retrieval and generative AI for accurate answers from PDFs.\n",
        "- Use lightweight models (`lightonai/GTE-ModernColBERT-v1`, `google/flan-t5-base`) for CPU compatibility.\n",
        "- Apply regex post-processing for roles (e.g., CEO names) and products (e.g., product lists) with deduplication.\n",
        "- Provide an interactive query interface with readable output.\n",
        "\n",
        "## Architecture\n",
        "- **Knowledge Base**: PDFs are loaded with `PyPDFLoader`, split into chunks (300 chars, 50-char overlap) using `RecursiveCharacterTextSplitter`, and stored with source tracking for company filtering.\n",
        "- **Semantic Layer**: Chunks and queries are embedded with `lightonai/GTE-ModernColBERT-v1`.\n",
        "- **Retrieval**: `retrieve.ColBERT` fetches 15 chunks, reranked to 3 by `rank.rerank`.\n",
        "- **Augmentation**: Top 3 chunks are combined with the query via `PromptTemplate`.\n",
        "- **Generation**: `google/flan-t5-base` generates answers, post-processed for roles, products, or lists.\n",
        "\n",
        "## Updates\n",
        "- Improved role extraction regex for names (e.g., 'Dr. Elena Ruiz' for CEO).\n",
        "- Refined product regex to filter non-products (e.g., 'Compliance').\n",
        "- Added company filtering (QuantumCore/NeoCompute) based on query keywords.\n",
        "- Enhanced output formatting with clear sections, suppressed progress bars, and concise progress messages.\n",
        "\n",
        "## Setup\n",
        "- **Dependencies**: `pylate`, `langchain`, `transformers`, `google-colab`, `pypdf`, `hf_xet`.\n",
        "- **Environment**: Google Colab, CPU-friendly.\n",
        "- **Datasets**: `QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf` (quantum computing), `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf` (AI hardware/software).\n",
        "\n",
        "## Instructions\n",
        "1. Cell 1: Install libraries.\n",
        "2. Cell 2: Import libraries.\n",
        "3. Cell 3: Define RAG pipeline.\n",
        "4. Cell 4: Process PDFs.\n",
        "5. Cell 5: Run interactive queries with formatted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLspdHdKrBcK"
      },
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Installs Python libraries for the RAG pipeline, ensuring compatibility in Google Colab. Includes `pylate` (ColBERT embeddings/retrieval), `langchain` (document processing), `transformers` (FLAN-T5), `google-colab` (Colab utilities), and `langchain-community`, `pypdf`, `hf_xet` for PDF processing and Hugging Face integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7Ylqb4rBcL"
      },
      "outputs": [],
      "source": [
        "!pip install pylate langchain transformers google-colab\n",
        "!pip install -U langchain-community pypdf hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmi7zbRrBcL"
      },
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "Imports libraries for the pipeline and suppresses warnings for clean output. Includes `pylate` (embedding/retrieval), `langchain` (PDF loading, text splitting, prompts), `transformers` (FLAN-T5), `google.colab.files` (Colab uploads), `os` (file paths), `re` (regex), and suppresses `pypdf` warnings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkQq0lhyGP7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pylate import models, indexes, retrieve, rank\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "warnings.filterWarnings('ignore', category=UserWarning, module='pypdf._reader')\n",
        "warnings.filterWarnings('ignore', category=DeprecationWarning, module='pypdf._reader')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZALkUDKyfNn"
      },
      "source": [
        "## Cell 3: Define RAG Pipeline\n",
        "\n",
        "Defines the RAG pipeline functions:\n",
        "- `run_rag_pipeline`: Loads, chunks, embeds, and indexes PDFs, initializes retriever and generator, tracks source PDFs for company filtering.\n",
        "- `query_rag`: Encodes queries, retrieves/reranks chunks, generates answers, and post-processes for roles, products, or lists. Features:\n",
        "  - Company filtering (QuantumCore/NeoCompute).\n",
        "  - Improved role regex (e.g., CEO names).\n",
        "  - Refined product regex (excludes non-products).\n",
        "  - Suppressed progress bars, custom progress messages.\n",
        "\n",
        "Ensures valid JSON with proper string escaping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXZdkSmrBcM"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(pdf_paths):\n",
        "    \"\"\"Initialize the RAG pipeline by processing PDFs, creating embeddings, and setting up models.\"\"\"\n",
        "    try:\n",
        "        all_document_texts = []\n",
        "        all_document_ids = []\n",
        "        document_map = {}\n",
        "        current_doc_id = 0\n",
        "        document_sources = {}\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f'Processing PDF: {pdf_path}')\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            document_texts = [chunk.page_content for chunk in chunks]\n",
        "            document_ids = [str(i + current_doc_id) for i in range(len(document_texts))]\n",
        "            document_map.update(dict(zip(document_ids, document_texts)))\n",
        "            source_name = os.path.basename(pdf_path).split('_')[0]\n",
        "            document_sources.update({doc_id: source_name for doc_id in document_ids})\n",
        "            all_document_texts.extend(document_texts)\n",
        "            all_document_ids.extend(document_ids)\n",
        "            current_doc_id += len(document_texts)\n",
        "            print(f'Created {len(document_texts)} chunks from {pdf_path}')\n",
        "\n",
        "        print(f'Total chunks created: {len(all_document_texts)}')\n",
        "\n",
        "        model_name = 'lightonai/GTE-ModernColBERT-v1'\n",
        "        model = models.ColBERT(model_name_or_path=model_name)\n",
        "\n",
        "        index_folder = 'pylate-index'\n",
        "        index_name = 'pdf_index'\n",
        "        index = indexes.Voyager(index_folder=index_folder, index_name=index_name, override=True)\n",
        "\n",
        "        documents_embeddings = model.encode(\n",
        "            all_document_texts,\n",
        "            batch_size=32,\n",
        "            is_query=False,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "        index.add_documents(all_document_ids, documents_embeddings=documents_embeddings)\n",
        "\n",
        "        retriever = retrieve.ColBERT(index=index)\n",
        "\n",
        "        generator = pipeline('text2text-generation', model='google/flan-t5-base', max_length=300)\n",
        "\n",
        "        prompt_template = \"Using only the provided text, answer the user's question with a concise and accurate response. For questions about specific roles (e.g., CEO, CTO, CFO), return only the full name of the individual in that role. For questions about lists (e.g., products), return all items as a comma-separated list of names only. Exclude any details not directly relevant to the question, such as technical specifications, unless explicitly requested. If the answer is not in the text, respond with 'The answer could not be found in the text.'\\n\\nText: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "        return model, index, retriever, generator, PROMPT, document_map, document_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDFs: {e}')\n",
        "        return None\n",
        "\n",
        "def query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query):\n",
        "    \"\"\"Process a user query with retrieval, generation, and post-processing.\"\"\"\n",
        "    try:\n",
        "        queries = [query]\n",
        "\n",
        "        print('Encoding query...')\n",
        "        query_embedding = model.encode(\n",
        "            queries,\n",
        "            batch_size=32,\n",
        "            is_query=True,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "\n",
        "        print('Retrieving documents...')\n",
        "        top_k_initial = 15\n",
        "        initial_results = retriever.retrieve(queries_embeddings=query_embedding, k=top_k_initial)\n",
        "        retrieved_doc_ids = [result['id'] for result in initial_results[0]]\n",
        "\n",
        "        company = None\n",
        "        if 'quantumcore' in query.lower():\n",
        "            company = 'QuantumCore'\n",
        "        elif 'neocompute' in query.lower():\n",
        "            company = 'NeoCompute'\n",
        "        if company:\n",
        "            retrieved_doc_ids = [doc_id for doc_id in retrieved_doc_ids if document_sources.get(doc_id) == company]\n",
        "        retrieved_documents = [document_map[doc_id] for doc_id in retrieved_doc_ids]\n",
        "\n",
        "        print('Reranking documents...')\n",
        "        reranked_results = rank.rerank(\n",
        "            documents_ids=[retrieved_doc_ids],\n",
        "            queries_embeddings=query_embedding,\n",
        "            documents_embeddings=[model.encode(retrieved_documents, is_query=False, show_progress_bar=False)]\n",
        "        )\n",
        "\n",
        "        reranked_doc_ids = []\n",
        "        if reranked_results and isinstance(reranked_results[0], list):\n",
        "            for result in reranked_results[0]:\n",
        "                if isinstance(result, dict) and 'id' in result:\n",
        "                    reranked_doc_ids.append(result['id'])\n",
        "                elif isinstance(result, str):\n",
        "                    reranked_doc_ids.append(result)\n",
        "        else:\n",
        "            reranked_doc_ids = retrieved_doc_ids\n",
        "\n",
        "        reranked_documents = [document_map[doc_id] for doc_id in reranked_doc_ids]\n",
        "\n",
        "        max_context_length = 600\n",
        "        context = '\\n'.join(reranked_documents[:3])[:max_context_length]\n",
        "        prompt_text = PROMPT.format(context=context, question=query)\n",
        "\n",
        "        print('Generating answer...')\n",
        "        response = generator(prompt_text)[0]['generated_text']\n",
        "        answer = response.strip()\n",
        "\n",
        "        non_product_terms = {'Compliance', 'Cooling', 'Features', 'Storage', 'Networking', 'Frameworks', 'Uptime', 'Encryption', 'Certifications', 'Software'}\n",
        "        if any(role in query.lower() for role in ['ceo', 'cto', 'cfo', 'cio']):\n",
        "            role = next((r for r in ['CEO', 'CTO', 'CFO', 'CIO'] if r.lower() in query.lower()), None)\n",
        "            if role:\n",
        "                match = re.search(r'- ([^,]+?),\\s*'+role+r'\\s*:', context, re.IGNORECASE)\n",
        "                if match:\n",
        "                    answer = match.group(1).strip()\n",
        "                else:\n",
        "                    answer = 'The answer could not be found in the text.'\n",
        "        elif 'product' in query.lower():\n",
        "            product_names = re.findall(r'- (\\w+): (?:Quantum|Cloud-based|processing unit|platform|cryptographic security|cloud-native|security modules)', context, re.IGNORECASE)\n",
        "            product_names = [name for name in product_names if name not in non_product_terms]\n",
        "            if product_names:\n",
        "                answer = ', '.join(sorted(set(product_names)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        elif 'compliance' in query.lower():\n",
        "            compliance_standards = re.findall(r'- ([^\\n]+)', context, re.IGNORECASE)\n",
        "            compliance_standards = [std.strip() for std in compliance_standards if any(term in std for term in ['SOC', 'ISO', 'HIPAA', 'GDPR'])]\n",
        "            if compliance_standards:\n",
        "                answer = ', '.join(sorted(set(compliance_standards)))\n",
        "            else:\n",
        "                answer = 'The answer could not be found in the text.'\n",
        "        elif ', ' in answer:\n",
        "            items = set(answer.split(', '))\n",
        "            items = [item for item in items if item not in non_product_terms]\n",
        "            answer = ', '.join(sorted(items)) if items else 'The answer could not be found in the text.'\n",
        "\n",
        "        return context, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing query: {e}')\n",
        "        return None, 'Error processing query.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA4sXRQrBcM"
      },
      "source": [
        "## Cell 4: Process PDFs\n",
        "\n",
        "Specifies PDF paths (`QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf`, `NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf`) and initializes the RAG pipeline. Checks for missing files and prompts for uploads in Colab. Initializes ColBERT model, Voyager index, retriever, FLAN-T5 generator, prompt template, document map, and source tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_VGi9nrBcM"
      },
      "outputs": [],
      "source": [
        "pdf_paths = [\n",
        "    '/data/QuantumCore_Solutions_RAG_Demo_Dataset_v1.pdf',\n",
        "    '/data/NeoCompute_Technologies_RAG_Demo_Dataset_v2.pdf'\n",
        "]\n",
        "\n",
        "if not all(os.path.exists(pdf_path) for pdf_path in pdf_paths):\n",
        "    print('Please upload your PDF files (QuantumCore_v1.pdf and/or NeoCompute_v2.pdf):')\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "\n",
        "result = run_rag_pipeline(pdf_paths)\n",
        "if result:\n",
        "    model, index, retriever, generator, PROMPT, document_map, document_sources = result\n",
        "    print('RAG pipeline initialized successfully.')\n",
        "else:\n",
        "    print('Failed to initialize RAG pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Interactive Querying\n",
        "\n",
        "Provides an interactive query interface with formatted output. Features:\n",
        "- Clear sections with headers, horizontal lines, and indentation.\n",
        "- Suppressed progress bars, replaced with concise messages.\n",
        "- Example queries test roles, products, specifications, and compliance.\n",
        "\n",
        "**Example Queries**:\n",
        "- Who is the CEO of QuantumCore Solutions? → 'Dr. Elena Ruiz'\n",
        "- What are the products offered by NeoCompute Technologies? → 'NeoCloud, NeoSecure'\n",
        "- Who is the CIO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- Who is the CEO of NeoCompute Technologies? → 'The answer could not be found in the text.'\n",
        "- What is the qubit count of QubitCore? → '50-qubit superconducting architecture'\n",
        "- What compliance standards does NeoCompute follow? → 'SOC 2 Type II, ISO/IEC 27001, HIPAA, GDPR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run an interactive query loop with formatted output.\"\"\"\n",
        "    print('====================================')\n",
        "    print('RAG Query Interface')\n",
        "    print('====================================')\n",
        "    print('Enter your query (or type \"exit\" to quit):\\n')\n",
        "    while True:\n",
        "        query = input('Query: ')\n",
        "        if query.lower() == 'exit':\n",
        "            print('\\nExiting query interface.')\n",
        "            break\n",
        "        if not result:\n",
        "            print('\\nRAG pipeline not initialized. Please run Cell 4 first.')\n",
        "            break\n",
        "        context, answer = query_rag(model, index, retriever, generator, PROMPT, document_map, document_sources, query)\n",
        "        print('\\n====================================')\n",
        "        print(f'Query: {query}')\n",
        "        print('====================================')\n",
        "        print('\\n**Context Retrieved**:\\n')\n",
        "        print(f'    {context.replace(\"\\n\", \"\\n    \")}')\n",
        "        print('\\n---')\n",
        "        print('\\n**Answer**:\\n')\n",
        "        print(f'    {answer}')\n",
        "        print('\\n====================================\\n')\n",
        "        print('Enter your next query (or type \"exit\" to quit):\\n')\n",
        "\n",
        "interactive_query()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "nbformat": 4,
    "nbformat_minor": 0
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
